# Tensor_RT ä¸“é¢˜



# æœ¯è¯­/æ ¸å¿ƒæ¦‚å¿µ



# å‚è€ƒ

å®˜æ–¹æ–‡æ¡£: https://nvidia.github.io/TensorRT-LLM/latest/advanced/gpt-attention.html#chunked-context



# ç®€ä»‹

https://developer.nvidia.cn/tensorrt

NVIDIAÂ® TensorRTâ„¢ æ˜¯ä¸€ä¸ªå·¥å…·ç”Ÿæ€ç³»ç»Ÿï¼Œå¯ä¾›å¼€å‘è€…å®ç°é«˜æ€§èƒ½æ·±åº¦å­¦ä¹ æ¨ç†ã€‚TensorRT åŒ…æ‹¬æ¨ç†ç¼–è¯‘å™¨ã€è¿è¡Œæ—¶å’Œæ¨¡å‹ä¼˜åŒ–ï¼Œå¯ä¸ºç”Ÿäº§åº”ç”¨æä¾›ä½å»¶è¿Ÿå’Œé«˜ååé‡ã€‚TensorRT ç”Ÿæ€ç³»ç»ŸåŒ…æ‹¬ TensorRT ç¼–è¯‘å™¨ã€TensorRT-LLMã€TensorRT Model Optimizer å’Œ TensorRT Cloud

## å·¥ä½œåŸç†

ä¸ä»…ä½¿ç”¨ CPU çš„å¹³å°ç›¸æ¯”ï¼Œæ¨ç†é€Ÿåº¦æé«˜äº† 36 å€ã€‚

TensorRT åŸºäº NVIDIAÂ® CUDAÂ® å¹¶è¡Œç¼–ç¨‹æ¨¡å‹æ„å»ºï¼ŒåŒ…å«ç”¨äºä¼˜åŒ–åœ¨æ‰€æœ‰ä¸»è¦æ¡†æ¶ä¸Šè®­ç»ƒçš„ç¥ç»ç½‘ç»œæ¨¡å‹çš„åº“ï¼Œå¯¹è¿™äº›æ¨¡å‹è¿›è¡Œé«˜ç²¾åº¦æ ¡æ­£ä»¥è·å¾—è¾ƒä½çš„ç²¾åº¦ï¼Œå¹¶å°†å…¶éƒ¨ç½²åˆ°è¶…å¤§è§„æ¨¡æ•°æ®ä¸­å¿ƒã€å·¥ä½œç«™ã€ç¬”è®°æœ¬ç”µè„‘å’Œè¾¹ç¼˜è®¾å¤‡ã€‚TensorRT ä½¿ç”¨é‡åŒ–ã€å±‚å’Œå¼ é‡èåˆä»¥åŠå†…æ ¸è°ƒä¼˜ç­‰æŠ€æœ¯æ¥ä¼˜åŒ–æ¨ç†ã€‚

TensorRT ä¸ºä½¿ç”¨é‡åŒ–æ„ŸçŸ¥è®­ç»ƒæŠ€æœ¯è®­ç»ƒçš„æ¨¡å‹æä¾›è®­ç»ƒåé‡åŒ–å’Œæ”¯æŒï¼Œä»¥ä¼˜åŒ–æ·±åº¦å­¦ä¹ æ¨ç†çš„ FP8ã€FP4 å’Œæ•´æ•°æ ¼å¼ã€‚æ¨ç†ç²¾åº¦çš„é™ä½å¯æ˜¾è‘—é™ä½å»¶è¿Ÿï¼Œæ»¡è¶³è®¸å¤šå®æ—¶æœåŠ¡ä»¥åŠè‡ªä¸»å’ŒåµŒå…¥å¼åº”ç”¨ç¨‹åºçš„éœ€æ±‚







# kv cache

## å¸è½½åˆ°ä¸»æœºå†…å­˜

- pinå†…å­˜
- è€æ¶æ„æ•ˆæœä¸æ˜æ˜¾, æ–°æ¶æ„Grace-Hopperä¸Šä»CPUå¤åˆ¶kv_cacheåˆ°GPUæ˜¾å­˜ä¸Šéå¸¸å¿«

å¸è½½åˆ°ä¸»æœºå†…å­˜ä¼šå¢åŠ  kv ç¼“å­˜é‡ç”¨çš„å¯èƒ½æ€§ã€‚å¯¹äºä¼˜å…ˆçº§è¾ƒé«˜çš„ä»»åŠ¡ï¼ˆä¾‹å¦‚ä¼ æ’­å·²è¿è¡Œçš„è¯·æ±‚ï¼‰ï¼Œå¯é‡ç”¨å—ä¼šè¢«å¤åˆ¶åˆ°ä¸»æœºå†…å­˜çš„ç¼“å†²åŒºä¸­ï¼Œè€Œä¸æ˜¯è¢«é€å‡ºã€‚è¿™æå¤§åœ°æ‰©å±•äº†å¯ä¾›é‡ç”¨çš„å†…å­˜å®¹é‡ï¼Œä½¿å—èƒ½å¤Ÿæ›´é•¿æ—¶é—´åœ°ä¿æŒå¯é‡ç”¨çŠ¶æ€ã€‚å¦ä¸€æ–¹é¢ï¼Œå¸è½½å—ï¼ˆä»¥åŠå—é‡ç”¨åçš„åç»­åŠ è½½ï¼‰ä¼šäº§ç”Ÿä¸€äº›æˆæœ¬ï¼Œå› ä¸ºå¿…é¡»å°†å—ä» CPU å¤åˆ¶åˆ° GPU å†…å­˜ï¼Œåä¹‹äº¦ç„¶ã€‚åœ¨ Grace-Hopper æœºå™¨ä¸Šï¼Œè¿™ç§æˆæœ¬å¯ä»¥å¿½ç•¥ä¸è®¡ï¼Œå¹¶ä¸”è¶³å¤Ÿå°ï¼Œè¶³ä»¥ä¸ºé…å¤‡ Hopper GPU çš„ x86 æœºå™¨ä¸Šçš„è®¸å¤šç”¨ä¾‹å¸¦æ¥å‡€æ”¶ç›Šã€‚ç”±äº GPU å’Œä¸»æœºå†…å­˜ä¹‹é—´çš„è¿æ¥é€Ÿåº¦ï¼ˆç›¸å¯¹è¾ƒæ…¢ï¼‰ï¼Œå¸è½½ä¸å¤ªå¯èƒ½åœ¨è¾ƒæ—§çš„æ¶æ„ä¸Šå¸¦æ¥å¥½å¤„ã€‚

å¦‚æœæ‚¨æ­£åœ¨è¿è¡Œ gptManagerBenchmarkï¼Œåˆ™å¯ä»¥ä½¿ç”¨å‘½ä»¤è¡Œå¼€å…³å¯ç”¨å¸è½½ã€‚ä¾‹å¦‚ï¼š

```
gptManagerBenchmark --kv_host_cache_bytes 45000000000
```

**å°†åœ¨ä¸»æœºå†…å­˜ä¸­åˆ›å»ºä¸€ä¸ª 45 GiB çš„å¸è½½ç¼“å†²åŒºã€‚è¯·æ³¨æ„ï¼Œæ­¤ç¼“å†²åŒºæ˜¯å›ºå®šå†…å­˜ï¼Œåœ¨ x86 æœºå™¨ä¸Šåˆ†é…å¤§é‡å›ºå®šå†…å­˜å¯èƒ½éœ€è¦å¤§é‡æ—¶é—´ï¼ˆæ•°åç§’ï¼‰ã€‚è¿™æ˜¯ä¸€æ¬¡æ€§æˆæœ¬ã€‚**

å¦‚æœæ‚¨æ­£åœ¨è¿è¡Œ Triton æœåŠ¡å™¨ï¼Œåˆ™å¯ä»¥ä½¿ç”¨ kv_cache_host_memory_bytes å‚æ•°å¯ç”¨å¸è½½åˆ°ä¸»æœºå†…å­˜çš„åŠŸèƒ½ã€‚ä¾‹å¦‚ï¼Œå°†å…¶æ·»åŠ åˆ°æ¨¡å‹é…ç½®æ–‡ä»¶ä¸­å°†åœ¨ä¸»æœºå†…å­˜ä¸­åˆ›å»ºä¸€ä¸ª 45 GiB çš„å¸è½½ç¼“å†²åŒºã€‚

```
parameters: {
  key: "kv_cache_host_memory_bytes"
  value: {
    string_value: "45000000000"
  }
}
```



å¦‚æœæ‚¨ä½¿ç”¨ Executor API ç¼–å†™è‡ªå·±çš„åº”ç”¨ç¨‹åºï¼Œåˆ™å¯ä»¥`hostCacheSize=45000000000`åœ¨åˆ›å»º`KvCacheConfig`å¯¹è±¡æ—¶æ·»åŠ æ­¤åŠŸèƒ½ï¼Œä»¥å¯ç”¨å¸è½½åˆ°ä¸»æœºçš„åŠŸèƒ½ã€‚è¿™å°†åœ¨ä¸»æœºå†…å­˜ä¸­åˆ›å»ºä¸€ä¸ª 45 GiB çš„å¸è½½ç¼“å†²åŒºã€‚





# ä¸Šä¸‹æ–‡åˆ†å— context chunking

ä½ è¿™æ®µè¯å…¶å®æ˜¯åœ¨è§£é‡Š **å¤§æ¨¡å‹æ¨ç†ä¸­çš„â€œä¸Šä¸‹æ–‡åˆ†å—ï¼ˆcontext chunkingï¼‰â€æœºåˆ¶**ã€‚æˆ‘å¸®ä½ æ‹†å¼€è¯´æ˜ï¼š

------

### èƒŒæ™¯ï¼šåŸå§‹åšæ³•

- **ä¼ ç»Ÿæ¨ç†æµç¨‹**ï¼šåœ¨é¢„å¡«å……ï¼ˆprefillï¼‰é˜¶æ®µï¼ŒæŠŠæ‰€æœ‰è¾“å…¥ä¸Šä¸‹æ–‡ token ä¸€æ¬¡æ€§é€è¿›æ¨¡å‹ï¼Œç®—å®Œæ•´çš„ KV Cacheã€‚
- **é—®é¢˜**ï¼š
  1. è¾“å…¥å¤ªé•¿æ—¶ï¼ŒGPU å†…å­˜å’Œè®¡ç®—å‹åŠ›å¾ˆå¤§ï¼ˆprefill é˜¶æ®µå»¶è¿Ÿé«˜ï¼‰ã€‚
  2. KV-Cache æ„å»ºå—è¾“å…¥é•¿åº¦é™åˆ¶ï¼Œå®¹æ˜“ OOMã€‚
  3. ååç‡ï¼ˆthroughputï¼‰ä¸é«˜ï¼Œå› ä¸º batch ä¹‹é—´éš¾ä»¥å¤ç”¨ã€‚

------

### æ”¹è¿›ï¼šä¸Šä¸‹æ–‡åˆ†å—ï¼ˆcontext chunkingï¼‰

- **æ€è·¯**ï¼šæŠŠä¸Šä¸‹æ–‡æ‹†æˆå¤šä¸ª **chunk**ï¼ˆåˆ†å—ï¼‰ï¼Œè€Œä¸æ˜¯ä¸€æ¬¡æ€§è¾“å…¥ã€‚
- **å¥½å¤„**ï¼š
  1. **æ›´é«˜åå**ï¼šæ¯ä¸ª chunk å¤„ç†æ—¶ï¼Œå¯ä»¥å’Œå…¶ä»–è¯·æ±‚çš„ token ç»„ batchï¼Œæå‡æ•´ä½“ååç‡ã€‚
  2. **æ›´çµæ´»çš„è¾“å…¥é•¿åº¦**ï¼šä¸å†å—é™äºå•æ¬¡è¾“å…¥çš„ KV-Cache å¤§å°ï¼Œå¯ä»¥æ”¯æŒè¶…é•¿ä¸Šä¸‹æ–‡ã€‚
  3. **å†…å­˜å‹åŠ›æ›´å¹³è¡¡**ï¼šé€å—å¡«å…… KV-Cacheï¼Œé¿å…ä¸€æ¬¡æ€§çˆ†æ˜¾å­˜ã€‚

------

### æŠ€æœ¯è¦æ±‚

1. **éœ€è¦å¯ç”¨ FMHA paged KV-Cache**ï¼š
   - FMHA = Flash Multi-Head Attention
   - â€œpaged KV-Cacheâ€ = åˆ†é¡µçš„ KV ç¼“å­˜æœºåˆ¶ï¼ŒæŠŠ KV æŒ‰å›ºå®šå¤§å°çš„ block å­˜å‚¨åœ¨æ˜¾å­˜é‡Œã€‚
   - å¥½å¤„æ˜¯æ›´å¥½åœ°ç®¡ç†æ˜¾å­˜ï¼Œæ”¯æŒåŠ¨æ€æ‰©å±•ã€‚
2. **chunk å¤§å°è¦æ±‚**ï¼š
   - é™¤äº†æœ€åä¸€ä¸ªå—ï¼Œæ‰€æœ‰çš„ chunk é•¿åº¦å¿…é¡»æ˜¯ **KV-Cache block size çš„æ•´æ•°å€**ã€‚
   - è¿™æ ·èƒ½ä¿è¯ KV å­˜å‚¨å¯¹é½ï¼Œé¿å…æµªè´¹å’Œå¤æ‚ç®¡ç†ã€‚

------

### ç›´è§‚ç†è§£

å‡è®¾ï¼š

- KV block size = 16 tokens
- è¾“å…¥ä¸Šä¸‹æ–‡ = 4000 tokens

å¦‚æœä¸ç”¨åˆ†å—ï¼šä¸€æ¬¡æ€§å¤„ç† 4000 tokensï¼Œæ˜¾å­˜å‹åŠ›å·¨å¤§ã€‚
 å¦‚æœç”¨åˆ†å—ï¼š

- æ¯æ¬¡å¤„ç† 512 tokensï¼ˆ512 = 16 çš„å€æ•°ï¼‰
- å‰ 7 ä¸ª chunk = 7 Ã— 512 = 3584 tokens
- æœ€å 1 ä¸ª chunk = 416 tokensï¼ˆä¸å¿…æ˜¯å€æ•°ï¼‰ã€‚

è¿™æ ·å°±èƒ½ï¼š

- åˆ†é˜¶æ®µæ„å»º KV Cache
- ä¸­é€”å¯ä»¥å’Œå…¶ä»–è¯·æ±‚åˆå¹¶ batchï¼Œæ•´ä½“ GPU ååæ›´é«˜

------

ğŸ‘‰ æ€»ç»“ä¸€å¥ï¼š
 **ä¸Šä¸‹æ–‡åˆ†å—ï¼ˆcontext chunkingï¼‰= æŠŠé•¿è¾“å…¥æ‹†å°å—é…åˆ paged KV-Cache å­˜å‚¨ï¼Œæ—¢èƒ½æ”¯æŒæ— é™é•¿è¾“å…¥ï¼Œåˆèƒ½æå‡ååå’Œæ˜¾å­˜åˆ©ç”¨ç‡ã€‚**

------

è¦ä¸è¦æˆ‘ç»™ä½ ç”»ä¸ªå›¾ï¼Œå¯¹æ¯”ä¸€ä¸‹ **åŸå§‹ï¼ˆä¸€æ¬¡æ€§è¾“å…¥ï¼‰ vs åˆ†å—ï¼ˆchunkingï¼‰** çš„æµç¨‹ï¼Ÿ







# æ”¯æŒMNNL all2all

ä¸‹é¢è¿™æ¡ PR æ ‡é¢˜çš„æ„æ€å’Œä»·å€¼ï¼Œæˆ‘ç»™ä½ æ‹†å¼€è®²æ¸…æ¥šï¼Œå¹¶ç»™å‡ºè½åœ°è¦ç‚¹ä¸æ’éšœæ€è·¯ï¼š

# è¿™æ¡ PR åœ¨è¯´ä»€ä¹ˆï¼Ÿ

**feat: large-scale EP (part 1: Add MNNVL MoE A2A support) #3504**

* **large-scale EP**ï¼šé¢å‘å¤§è§„æ¨¡ **Expert Parallelismï¼ˆä¸“å®¶å¹¶è¡Œï¼‰** çš„æ”¹é€ ã€‚
* **part 1**ï¼šç³»åˆ—æ”¹åŠ¨çš„ç¬¬ä¸€æ­¥ï¼Œåé¢è¿˜ä¼šæœ‰ç»­é›†ã€‚
* **MNNVL**ï¼šNVIDIA **Multi-Node NVLink** çš„æœºæŸœ/æœºæ¶çº§ NVLink äº’è”ä½“ç³»ï¼ˆBlackwell/Grace + NVSwitch è·¨èŠ‚ç‚¹åŸŸï¼‰ã€‚åœ¨è¿™ç§ç³»ç»Ÿä¸Šï¼ŒGPU ä¹‹é—´ä¸ä»…æœºå†…æœ‰ NVLinkï¼Œ**è·¨èŠ‚ç‚¹** ä¹Ÿèƒ½èµ° NVLink åŸŸã€‚([NVIDIA Docs][1])
* **MoE A2A support**ï¼šä¸º **Mixture-of-Experts** çš„æ ¸å¿ƒé€šä¿¡ **All-to-Allï¼ˆA2Aï¼‰** åŠ ä¸Š MNNVL åœºæ™¯çš„æ”¯æŒ/ä¼˜åŒ–ï¼ŒæœåŠ¡çš„æ˜¯ **EP è·¯ç”±**ï¼ˆtoken åˆ†å‘ä¸å›æ”¶ï¼‰ã€‚A2A æ˜¯ MoE çš„ä¸»è¦ç“¶é¢ˆã€‚([USENIX][2], [arXiv][3])

è¿™æ¡æ”¹åŠ¨å‡ºç°åœ¨ **TensorRT-LLM** çš„ç‰ˆæœ¬æ›´æ–°è¯´æ˜ä¸­ï¼Œè¡¨æ˜å®˜æ–¹åœ¨ MoE ä¸Šæ–°å¢äº† â€œ**MNNVL MoE A2A support**â€ã€‚([GitHub][4], [NVIDIA GitHub][5])

# ä¸ºä»€ä¹ˆé‡è¦ï¼Ÿ

1. **MoE çš„å‘½é—¨åœ¨ A2A**ï¼šæ¯ä¸ª MoE å±‚å‰å‘è¦åšä¸¤æ¬¡ A2Aï¼Œåå‘å†ä¸¤æ¬¡ï¼›å®ƒå¸¸å¸¸å‹ä½è®¡ç®—ï¼Œæˆä¸ºååä¸Šé™ã€‚æŠŠ A2A æ”¾åˆ° **MNNVL åŸŸ**ï¼Œæœ‰æœºä¼šæ˜¾è‘—é™ä½æ—¶å»¶å’Œæ‹¥å¡ã€‚([USENIX][2])
2. **æ‹“æ‰‘æ›´â€œè¿‘â€**ï¼šMNNVL æŠŠå¤šèŠ‚ç‚¹ GPU çº³å…¥åŒä¸€ NVLink åŸŸï¼Œ**å¸¦å®½/å»¶è¿Ÿ** ä¼˜äºä¼ ç»Ÿ IB/RoCEï¼Œåªè¦è½¯ä»¶æ ˆï¼ˆNCCL/UCX/TRT-LLMï¼‰æ­£ç¡®å¯ç”¨ï¼Œå°±èƒ½è®© EP çš„ A2A èµ°æ›´å¿«çš„è·¯å¾„ã€‚([NVIDIA Docs][1])
3. **å®˜æ–¹æ ˆæ‰“é€š**ï¼šTRT-LLM å¢å¼º + NCCL/MNNVL å¼€å…³é…åˆï¼Œæ„å‘³ç€ä½ åœ¨ **å¤§æ¨¡å‹æ¨ç†/è®­ç»ƒ** çš„ MoE EP ä¸Šæ›´å®¹æ˜“æ‹¿åˆ°è§„æ¨¡åŒ–æ”¶ç›Šã€‚([GitHub][4], [NVIDIA Docs][6])

# è¿™é€šå¸¸åŒ…å«å“ªäº›æŠ€æœ¯æ”¹åŠ¨ï¼Ÿ

* **A2A ç®—å­åœ¨ MNNVL æ‹“æ‰‘ä¸Šçš„å®ç°/è·¯å¾„é€‰æ‹©**ï¼ˆå¯èƒ½ä¼˜å…ˆ NVLink åŸŸã€é¿å…å¤šçº§ç½‘å¡å‡ºåŸŸï¼‰ã€‚
* **NCCL/UCX çš„åç«¯é€‚é…ä¸å¯ç”¨**ï¼ˆä¾‹å¦‚ MNNVL ç›¸å…³ env å¼€å…³ï¼Œå†…å­˜å¥æŸ„/IMEX åŸŸå‡†å¤‡ï¼‰ã€‚([NVIDIA Docs][6])
* **è·¯ç”±/å®¹é‡/é‡æ’**ï¼šMoE token çš„ pack/unpackã€paddingã€capacity factor ä¸è´Ÿè½½å‡è¡¡å¯¹ A2A ä»£ä»·çš„è¿é”å½±å“ï¼ˆéƒ¨åˆ†åœ¨åç»­ part é‡Œç»§ç»­ä¼˜åŒ–ï¼‰ã€‚
* **è°ƒåº¦é‡å **ï¼šä¸ compute overlapã€æµæ°´ä¸å¼ é‡å¹¶è¡Œçš„ååŒï¼ˆTRT-LLM æåˆ° overlap scheduler ç­‰èƒ½åŠ›ï¼‰ã€‚([NVIDIA GitHub][7])

# è½åœ°æ€ä¹ˆç”¨ï¼ˆå®æ“æç¤ºï¼‰

> ä»¥ **TRT-LLM + NCCL** çš„ MoE éƒ¨ç½²ä¸ºä¾‹ï¼ˆè®­ç»ƒ/æ¨ç†äºŒè€…æ€è·¯ç›¸è¿‘ï¼‰ï¼š

1. **ç¡¬ä»¶/é©±åŠ¨å‰ç½®**

   * ç¡®è®¤å¹³å°æ˜¯ **MNNVL**ï¼ˆå¦‚ GB200/Blackwell çš„å¤šèŠ‚ç‚¹ NVLink æœºæ¶ï¼‰ï¼Œé©±åŠ¨å’Œå›ºä»¶æ»¡è¶³è¦æ±‚ã€‚([NVIDIA Docs][1])
2. **å¯ç”¨ NCCL çš„ MNNVL æ”¯æŒ**

   * å…³é”®ç¯å¢ƒå˜é‡ï¼š`NCCL_MNNVL_ENABLE=1`ï¼ˆè¿˜éœ€è¦ `NCCL_CUMEM_ENABLE=1`ï¼Œå¹¶ç¡®ä¿ IMEX åŸŸé…ç½®æ­£ç¡®ï¼‰ã€‚([NVIDIA Docs][6])
3. **å¼€å¯ TRT-LLM çš„ MoE/MNNVL è·¯å¾„**

   * ä½¿ç”¨åŒ…å« â€œ**MNNVL MoE A2A support**â€ çš„ TRT-LLM ç‰ˆæœ¬ï¼›æŒ‰ release notes çš„ MoE æŒ‡å—/æ ·ä¾‹é…ç½® EP ç»´åº¦ä¸ A2A ç®—å­ã€‚([GitHub][4], [NVIDIA GitHub][5])
4. **UCX/NVLink è°ƒä¼˜ï¼ˆå¦‚ä½¿ç”¨ UCX è·¯å¾„ï¼‰**

   * éµå¾ªå®˜æ–¹å¤šèŠ‚ç‚¹ NVLink è°ƒä¼˜å»ºè®®ï¼ˆå†…å­˜æ³¨å†Œã€ä¼ è¾“é˜ˆå€¼ã€æ‹¥å¡ç­–ç•¥ï¼‰ã€‚([NVIDIA Docs][8])
5. **åŸºå‡†æµ‹è¯•ä¸å¯¹æ¯”**

   * å¯¹æ¯” **IB/RoCE vs MNNVL** çš„ A2A å»¶è¿Ÿ/ååï¼›å®æµ‹ batch/token åˆ†å¸ƒä¸‹çš„ç«¯åˆ°ç«¯ QPSã€TTFTã€P50/P99ã€‚
   * å…³æ³¨æ‰€æœ‰ EP ç»´åº¦ï¼ˆexpert æ•°ã€capacityã€top-kã€routerï¼‰å¯¹ A2A ä½“é‡çš„å½±å“ã€‚([USENIX][2])

# å¸¸è§å‘ä½ï¼ˆæ’éšœæ¸…å•ï¼‰

* **A2A ä»èµ°åˆ°äº† IB**ï¼šå¤šå›  MNNVL æœªæˆåŠŸå¯ç”¨ï¼ˆç¯å¢ƒå˜é‡/é©±åŠ¨/IMEX åŸŸï¼‰ï¼Œæˆ–é€šè®¯åº“å›é€€è·¯å¾„ã€‚æ£€æŸ¥ NCCL æ—¥å¿—ä¸ topo dumpã€‚([NVIDIA Docs][6])
* **å¸¦å®½æ²¡æœ‰æŠ¬èµ·æ¥**ï¼šA2A å’Œå…¶å®ƒé›†åˆé€šä¿¡ï¼ˆå¦‚ DP çš„ AllReduceï¼‰**ç«äº‰**ï¼›éœ€è¦è°ƒåº¦ä¸ overlapï¼Œé”™å³°æˆ–åˆ†æ‰¹æ¬¡ token-routingã€‚([USENIX][2])
* **EP ä¸å‡è¡¡å¯¼è‡´æ”¾å¤§ A2A**ï¼šrouter è´Ÿè½½ä¸å‡ã€capacity è¿‡å°å¼•å‘ä¸¢åŒ…/å›é€€ï¼›éœ€è¦è°ƒè·¯ç”±/æ­£åˆ™é¡¹ä¸ capacity factorã€‚([USENIX][2])

# ç»™é€šä¿¡/ç³»ç»Ÿä¼˜åŒ–å·¥ç¨‹å¸ˆçš„å»ºè®®

* **åº¦é‡é‡ç‚¹**ï¼šA2A æ¶ˆæ¯å¤§å°åˆ†å¸ƒã€æµé‡å³°å€¼ã€å¹¶å‘åº¦ã€ä¸ compute çš„é‡å åº¦ã€è·¨åŸŸè·³æ•°ã€‚
* **æ‹“æ‰‘æ˜ å°„**ï¼šä¸“å®¶æ”¾ç½®ä¸ NVLink åŸŸäº²å’Œï¼ˆæŠŠæµé‡æœ€å¤§çš„ä¸“å®¶å¯¹å°±è¿‘æ”¾ç½®ï¼Œå‡å°‘è·¨åŸŸ hopï¼‰ã€‚
* **å¤šå±‚æ¬¡ä¼˜åŒ–**ï¼š

  * ç®—å­å±‚ï¼štoken é‡æ’/èšåˆï¼ˆpack/unpackï¼‰ã€å‹ç¼©/é‡åŒ–ï¼ˆè‹¥å¯é ï¼‰ã€‚
  * è¿è¡Œæ—¶å±‚ï¼šA2A åˆ†ç‰‡ä¸æµæ°´é‡å ã€ä¼˜å…ˆçº§/credit ç®¡ç†ã€‚
  * é€šè®¯åº“å±‚ï¼šNCCL/UCX å‚æ•°ã€chunk/pipeline å¤§å°ã€é—¨é™ã€‚([NVIDIA Docs][8])

---

**ä¸€å¥è¯å°ç»“**ï¼š
è¿™æ¡ PR æ ‡å¿—ç€ **TensorRT-LLM å·²å¼€å§‹é¢å‘ MNNVL æ‹“æ‰‘ä¼˜åŒ– MoE çš„ A2A/EP** åœºæ™¯ã€‚å¯¹å¤§è§„æ¨¡ MoEï¼ˆè®­ç»ƒ/æ¨ç†ï¼‰æ¥è¯´ï¼Œè¿™æ˜¯æŠŠæœ€å¤§é€šä¿¡ç“¶é¢ˆè¿ç§»åˆ° **æ›´é«˜å¸¦å®½ã€æ›´ä½æ—¶å»¶** çš„ NVLink è·¨èŠ‚ç‚¹åŸŸä¸Šçš„å…³é”®ä¸€æ­¥ï¼›åªè¦é…å¥½ NCCL çš„ MNNVL å¼€å…³å¹¶æ®æ­¤è°ƒåº¦/æ‹“æ‰‘æ˜ å°„ï¼Œé€šå¸¸èƒ½æ‹¿åˆ°å¯è§‚çš„ç«¯åˆ°ç«¯æ”¶ç›Šã€‚([GitHub][4], [NVIDIA GitHub][5], [NVIDIA Docs][1], [USENIX][2])

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥æ ¹æ®ä½ çš„ **GPU/NIC/MNNVL æ‹“æ‰‘ä¸ MoE é…ç½®**ï¼Œç»™å‡ºä¸€ä»½**å…·ä½“çš„ NCCL/UCX/env å‚æ•°è¡¨ + åŸºå‡†è„šæœ¬**ï¼Œç›´æ¥è·‘å‡º A2A çš„å¯¹æ¯”æ›²çº¿ã€‚

[1]: https://docs.nvidia.com/multi-node-nvlink-systems/mnnvl-user-guide/overview.html?utm_source=chatgpt.com "Overview â€” MNNVL User Guide"
[2]: https://www.usenix.org/system/files/atc23-li-jiamin.pdf?utm_source=chatgpt.com "Accelerating Distributed MoE Training and Inference with ..."
[3]: https://arxiv.org/html/2505.09764v1?utm_source=chatgpt.com "FLASH: Fast All-to-All Communication in GPU Clusters"
[4]: https://github.com/NVIDIA/TensorRT-LLM/releases?utm_source=chatgpt.com "Releases Â· NVIDIA/TensorRT-LLM"
[5]: https://nvidia.github.io/TensorRT-LLM/release-notes.html?utm_source=chatgpt.com "Release Notes â€” TensorRT-LLM - GitHub Pages"
[6]: https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/env.html?utm_source=chatgpt.com "Environment Variables â€” NCCL 2.27.5 documentation"
[7]: https://nvidia.github.io/TensorRT-LLM/_sources/release-notes.md.txt?utm_source=chatgpt.com "release-notes.md.txt - GitHub Pages"
[8]: https://docs.nvidia.com/multi-node-nvlink-systems/multi-node-tuning-guide/ucx.html?utm_source=chatgpt.com "UCX - Communication Libraries"





# all2allé€šé“é€šä¿¡å™¨

ä½ è¿™æ®µä»£ç æ˜¯ä¸€ä¸ª **C++ æ¨¡æ¿ç±»å®šä¹‰çš„å¼€å¤´**ï¼Œæˆ‘å¸®ä½ è§£æä¸€ä¸‹å®ƒçš„å«ä¹‰ï¼š

```cpp
template <bool isSender>
class AllToAllChannelCommunicator : public AllToAllChannelCommunicatorBase
{
    // class body here
};
```

### é€ç‚¹è§£é‡Š

1. **æ¨¡æ¿å‚æ•°**

   ```cpp
   template <bool isSender>
   ```

   - è¿™é‡Œå®šä¹‰äº†ä¸€ä¸ªå¸ƒå°”ç±»å‹çš„éç±»å‹æ¨¡æ¿å‚æ•°ï¼ˆNTTP, Non-Type Template Parameterï¼‰ã€‚
   - `isSender` åœ¨ç¼–è¯‘æœŸå°±èƒ½ç¡®å®šæ˜¯ `true` è¿˜æ˜¯ `false`ï¼Œå› æ­¤ç¼–è¯‘å™¨å¯ä»¥ç”Ÿæˆä¸¤å¥—ä¸åŒçš„ç±»å®ç°ï¼š
     - `AllToAllChannelCommunicator<true>`
     - `AllToAllChannelCommunicator<false>`

   è¿™ç§å†™æ³•å¸¸ç”¨äºåœ¨ç¼–è¯‘æœŸåŒºåˆ†ã€Œå‘é€æ–¹ã€å’Œã€Œæ¥æ”¶æ–¹ã€çš„é€»è¾‘ï¼Œå®ç° **é›¶è¿è¡Œæ—¶å¼€é”€** çš„åˆ†æ”¯ã€‚

2. **ç»§æ‰¿å…³ç³»**

   ```cpp
   : public AllToAllChannelCommunicatorBase
   ```

   - `AllToAllChannelCommunicator` ç»§æ‰¿è‡ª `AllToAllChannelCommunicatorBase`ã€‚
   - `Base` ç±»å¯èƒ½æä¾›äº†é€šç”¨çš„æ¥å£ï¼ˆå¦‚åˆå§‹åŒ–ã€èµ„æºç®¡ç†ã€é€šé“æŠ½è±¡ï¼‰ã€‚
   - å­ç±»åˆ™æ ¹æ® `isSender` é€‰æ‹©æ€§å®ç°ã€Œå‘é€æ•°æ®ã€æˆ–ã€Œæ¥æ”¶æ•°æ®ã€çš„é€»è¾‘ã€‚

3. **å¯èƒ½çš„å®ç°æ¨¡å¼**

   - é€šè¿‡ `if constexpr (isSender)` æˆ– **æ¨¡æ¿ç‰¹åŒ–** å®ç°ä¸åŒé€»è¾‘ã€‚ä¾‹å¦‚ï¼š

   ```cpp
   template <bool isSender>
   class AllToAllChannelCommunicator : public AllToAllChannelCommunicatorBase {
   public:
       void communicate() {
           if constexpr (isSender) {
               send();
           } else {
               receive();
           }
       }
   
   private:
       void send() { /* sender logic */ }
       void receive() { /* receiver logic */ }
   };
   ```

   æˆ–è€…ä½¿ç”¨ **å…¨ç‰¹åŒ–**ï¼š

   ```cpp
   template <>
   class AllToAllChannelCommunicator<true> : public AllToAllChannelCommunicatorBase {
   public:
       void communicate() { send(); }
   private:
       void send() { /* sender logic */ }
   };
   
   template <>
   class AllToAllChannelCommunicator<false> : public AllToAllChannelCommunicatorBase {
   public:
       void communicate() { receive(); }
   private:
       void receive() { /* receiver logic */ }
   };
   ```

------

ğŸ‘‰ æˆ‘çŒœè¿™ä¸ªç±»å¯èƒ½æ˜¯ **åˆ†å¸ƒå¼é€šä¿¡/è®­ç»ƒæ¡†æ¶** é‡Œçš„ç»„ä»¶ï¼Œç”¨æ¥åš **All-to-All** é€šä¿¡é€šé“çš„æŠ½è±¡ï¼Œ`isSender` ç”¨äºåŒºåˆ†è§’è‰²ã€‚

è¦ä¸è¦æˆ‘å¸®ä½ å±•å¼€ä¸€ä¸ªå¯èƒ½çš„å®Œæ•´å®ç°ï¼ˆå¸¦ `send/recv` é€»è¾‘å’Œ `if constexpr` ä¼˜åŒ–ï¼‰ï¼Œè¿™æ ·èƒ½æ›´ç›´è§‚åœ°çœ‹åˆ°å®ƒæ€ä¹ˆç”¨åœ¨ All-to-All é€šä¿¡é‡Œï¼Ÿ



# AllToAllChannelCommunicator è¯¦è§£

éå¸¸å¥½ï¼Œè¿™æ®µä»£ç æ¯”ä¹‹å‰å¤šäº†å®Œæ•´çš„æˆå‘˜å’Œæ„é€ å‡½æ•°ï¼Œå·²ç»èƒ½çœ‹å‡ºå®ƒæ˜¯ä¸€ä¸ª **CUDA Kernel å†…è®¾å¤‡ç«¯é€šä¿¡ç±»**ï¼Œæˆ‘å¸®ä½ é€å±‚è§£æä¸€ä¸‹ï¼š

------

### 1. æ¨¡æ¿å£°æ˜

```cpp
template <bool isSender>
class AllToAllChannelCommunicator : public AllToAllChannelCommunicatorBase
```

- `isSender`ï¼šç¼–è¯‘æœŸåŒºåˆ†å‘é€æ–¹ / æ¥æ”¶æ–¹é€»è¾‘ã€‚
- ç»§æ‰¿è‡ª `AllToAllChannelCommunicatorBase`ï¼Œè¯´æ˜åº•å±‚æœ‰é€šç”¨çš„æ¥å£æˆ–å…±äº«é€»è¾‘ã€‚

------

### 2. æˆå‘˜å˜é‡åˆ†ç±»

#### (1) **çº¿ç¨‹/warp æ‹“æ‰‘ä¿¡æ¯**

```cpp
int const tid;      // å½“å‰ primitives group ä¸­çš„çº¿ç¨‹ ID
int const nthreads; // primitives group ä¸­æ€»çº¿ç¨‹æ•°
int const wid;      // å½“å‰çº¿ç¨‹åœ¨ warp å†…çš„ lane ç´¢å¼•
int const warp;     // å½“å‰çº¿ç¨‹åœ¨ group å†…çš„ warp ç´¢å¼•
int const group;    // primitives group index
int const channel;  // å½“å‰ä½¿ç”¨çš„é€šé“ç¼–å·
int const channelCount; // æ€»é€šé“æ•°é‡
bool const flagThread;  // æ¯8ä¸ªçº¿ç¨‹ä¸­æœ€åä¸€ä¸ª (tid % 8 == 7)ï¼Œå¯èƒ½ç”¨äºåšåŒæ­¥/æ ‡è®°
```

- `tid = threadIdx.x`
- `wid = tid % WARP_SIZE`
- `warp = tid / WARP_SIZE`
- `group = threadIdx.y`
- `channel = blockIdx.y`
- `peerRank = blockIdx.x * GROUP_COUNT_PER_BLOCK + threadIdx.y`

ğŸ‘‰ è¿™äº›å®šä¹‰å’Œ **åˆ†å¸ƒå¼ All-to-All é€šä¿¡çš„çº¿ç¨‹æ‹“æ‰‘**å¼ºç›¸å…³ï¼Œå…¸å‹æ¨¡å¼æ˜¯ï¼š

- ä¸€ä¸ª block è¦†ç›–ä¸€ä¸ª **rank group**ï¼ˆæ¯”å¦‚ MoE expert groupï¼‰ã€‚
- `threadIdx.y` åŒºåˆ† **group å†…çš„å­ç»„**ã€‚
- `blockIdx.y` åŒºåˆ† **é€šé“å·**ã€‚
- `blockIdx.x` ç»“åˆ `threadIdx.y` ç¡®å®š peer rankã€‚

------

#### (2) **é€šä¿¡/æ•°æ®åˆ†å¸ƒç›¸å…³**

```cpp
const MoeEpWorldInfo worldInfo;   // å…¨å±€ MoE world ä¿¡æ¯ (rank æ•°ã€å¹¶è¡Œæ‹“æ‰‘ç­‰)
const MoeCommWorkspace workspace; // é€šä¿¡ä½¿ç”¨çš„å·¥ä½œåŒº (buffer, queue ç­‰)
const SendRecvDataInfo sendRecvDataInfo; // æ¯ä¸ª peer çš„ send/recv å¤§å°
const SendRecvDispls dataDispls;  // å„ peer çš„æ•°æ®åç§»
int peerRank;                     // å¯¹ç«¯ rank id
```

è¿™äº›æ˜¯ **All-to-All é€šä¿¡è°ƒåº¦æ‰€éœ€çš„å…ƒæ•°æ®**ï¼Œå‘Šè¯‰æ¯ä¸ªçº¿ç¨‹è¦ç»™å“ªä¸ª rank å‘é€æ•°æ®ï¼Œæ•°æ®çš„èµ·å§‹ä½ç½®å’Œé•¿åº¦ã€‚

------

#### (3) **FIFO æœºåˆ¶ (ç¯å½¢é˜Ÿåˆ—)**

```cpp
MoeCommFifoConnInfo* fifoConnInfoPtr; // FIFO è¿æ¥ä¿¡æ¯ (å¯èƒ½æ˜¯æè¿°ç¬¦ã€é—¨æ§æœºåˆ¶)
uint64_t* fifoBasePtr; // FIFO buffer çš„åŸºå€
uint64_t step;         // å½“å‰é€šä¿¡ step (å‘é€/æ¥æ”¶çš„åºå·)
uint64_t tailStepCache;// ç¼“å­˜çš„å°¾éƒ¨ step
uint64_t regs[U64_DATA_REG_PER_THREAD]; // æ¯çº¿ç¨‹ç”¨äºå¯„å­˜å™¨ç¼“å­˜çš„æ•°æ®
uint64_t* stepFifoEntryPtr; // å½“å‰ step åœ¨ FIFO ä¸­çš„å…¥å£
```

ğŸ‘‰ è¿™äº›å˜é‡æ„å‘³ç€è¿™ä¸ªé€šä¿¡å®ç°é‡‡ç”¨äº† **åŸºäº FIFO çš„æ— é”ç¯å½¢ç¼“å†²åŒºæœºåˆ¶**ï¼Œé€šå¸¸ç”¨äº GPU å†…æ ¸é—´ / å¤šçº¿ç¨‹é€šä¿¡ã€‚

- `step` æ˜¯ç”Ÿäº§è€…/æ¶ˆè´¹è€…è¿›åº¦ï¼ˆç±»ä¼¼ NCCL é‡Œçš„ `head/tail`ï¼‰ã€‚
- `flagThread`ï¼ˆæ¯8ä¸ªçº¿ç¨‹é‡Œæœ€åä¸€ä¸ªï¼‰å¯èƒ½æ˜¯è´Ÿè´£å†™å…¥åŒæ­¥ flag åˆ° FIFOï¼Œé¿å…æ‰€æœ‰çº¿ç¨‹éƒ½å†™ã€‚

------

#### (4) **group/slice èŒƒå›´**

```cpp
int groupStartIndice;
int groupEndIndice;

int sliceStartIndice;
int sliceEndIndice;
```

- è¿™äº›å¯èƒ½ç”¨äºç¡®å®š **å½“å‰ group å†…çš„èµ·å§‹/ç»“æŸçº¿ç¨‹**ï¼Œä»¥åŠ **æ•°æ®åˆ†ç‰‡èŒƒå›´**ã€‚
- å…¸å‹åœºæ™¯ï¼šAll-to-All é€šä¿¡ä¼šæŠŠæ•°æ®åˆ‡æˆè‹¥å¹² sliceï¼Œç”±ä¸åŒ group è´Ÿè´£ã€‚

------

#### (5) **å…±äº«å†…å­˜ buffer**

```cpp
GroupSharedBuffer* groupSharedBuffer;
```

- ç”¨äºåŒä¸€ä¸ª group å†… warp/çº¿ç¨‹åä½œæ—¶ï¼Œäº¤æ¢ä¸´æ—¶æ•°æ®ã€‚
- å…¸å‹åœºæ™¯ï¼šwarp å†… coalesced load/storeï¼Œæˆ–è€…ä¸­è½¬ bufferã€‚

------

### 3. æ„é€ å‡½æ•°

```cpp
__inline__ __device__ AllToAllChannelCommunicator(
    MoeEpWorldInfo const& worldInfo,
    MoeCommWorkspace workspace,
    SendRecvDataInfo sendRecvDataInfo,
    SendRecvDispls dataDispls,
    GroupSharedBuffer* groupSharedBuffer,
    int channelCount)
```

åˆå§‹åŒ–åˆ—è¡¨é‡Œç»‘å®šäº† CUDA å†…å»ºå˜é‡ï¼š

- `tid(threadIdx.x)`
- `wid(threadIdx.x % WARP_SIZE)`
- `warp(threadIdx.x / WARP_SIZE)`
- `peerRank(blockIdx.x * GROUP_COUNT_PER_BLOCK + threadIdx.y)`
- `group(threadIdx.y)`
- `channel(blockIdx.y)`
- `flagThread(threadIdx.x % 8 == 7)`

ğŸ‘‰ è¿™è¯´æ˜ï¼š

- **blockIdx.x**ï¼šæ˜ å°„åˆ° peer rank åˆ†ç»„ã€‚
- **blockIdx.y**ï¼šæ˜ å°„åˆ°é€šä¿¡é€šé“ã€‚
- **threadIdx.y**ï¼šåŒºåˆ† group å†…çº¿ç¨‹ç»„ã€‚
- **threadIdx.x**ï¼šåœ¨çº¿ç¨‹ç»„å†…ç”¨äº warp å’Œ laneã€‚

------

### 4. æ–¹æ³•

```cpp
__inline__ __device__ uint64_t getFlag()
{
    return step + 1;
}
```

ç®€å•çš„ flag ç”Ÿæˆå‡½æ•°ï¼Œç”¨äº **å†™å…¥ FIFO æ ‡å¿—**ã€‚

- æ¯”å¦‚å‘é€æ•°æ®æ—¶ï¼ŒæŠŠ `flag = step+1` å†™å…¥ FIFO çš„å¯¹åº” entryï¼Œé€šçŸ¥æ¥æ”¶æ–¹æ•°æ® readyã€‚
- è¿™ç§æ–¹å¼é¿å…äº†é‡ç”¨è€çš„ flag å€¼ï¼ˆæ°¸è¿œå•è°ƒé€’å¢ï¼‰ã€‚

------

### æ€»ç»“

è¿™ä¸ªç±»æ˜¯ä¸€ä¸ª **CUDA å†…æ ¸çº§åˆ«çš„ All-to-All é€šé“é€šä¿¡æŠ½è±¡**ï¼Œå…³é”®ç‚¹æ˜¯ï¼š

1. æ¨¡æ¿å‚æ•° `isSender` åœ¨ç¼–è¯‘æœŸåŒºåˆ†ã€Œå‘é€æ–¹ã€/ã€Œæ¥æ”¶æ–¹ã€é€»è¾‘ã€‚
2. æ„é€ å‡½æ•°ç»‘å®šäº† `blockIdx / threadIdx`ï¼Œå°† GPU grid æ˜ å°„åˆ° **é€šä¿¡ç»„ (group)ã€é€šé“ (channel)ã€peer rank**ã€‚
3. ä½¿ç”¨ **FIFO ç¯å½¢é˜Ÿåˆ—** (`fifoBasePtr`, `step`, `flagThread`) æ¥å®ç°è·¨çº¿ç¨‹/warp çš„æ— é”é€šä¿¡ã€‚
4. æä¾›å¯„å­˜å™¨ç¼“å­˜ `regs[]` å’Œå…±äº«å†…å­˜ buffer `groupSharedBuffer`ï¼Œç”¨äºåŠ é€Ÿæ•°æ®æ¬è¿ã€‚

------

è¦ä¸è¦æˆ‘å¸®ä½  **å±•å¼€ä¸€ä¸ªä¼ªä»£ç çš„ send/recv å®ç°**ï¼ˆç”¨ `if constexpr (isSender)`ï¼‰ï¼Œè¿™æ ·ä½ èƒ½æ›´ç›´è§‚åœ°çœ‹åˆ°è¿™ä¸ªç±»æ€ä¹ˆé©±åŠ¨ All-to-All é€šä¿¡ï¼Ÿ







# mnnvlå†…å­˜å•å…ƒæµ‹è¯•è§£æ

æˆ‘æ¥è§£é‡Šè¿™ä¸ªæµ‹è¯•æ–‡ä»¶ `test_moe_alltoall.py` çš„æ ¸å¿ƒæ¦‚å¿µå’Œæµ‹è¯•é€»è¾‘ã€‚

## MoE All-to-All é€šä¿¡çš„æ ¸å¿ƒæ¦‚å¿µ

### 1. **ä»€ä¹ˆæ˜¯MoE All-to-Allé€šä¿¡ï¼Ÿ**

MoEï¼ˆMixture of Expertsï¼‰All-to-Allé€šä¿¡æ˜¯åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­ï¼Œå½“æ¯ä¸ªtokenéœ€è¦é€‰æ‹©top-kä¸ªä¸“å®¶æ—¶ï¼Œä¸åŒGPUèŠ‚ç‚¹ä¹‹é—´äº¤æ¢æ•°æ®çš„é€šä¿¡æ¨¡å¼ã€‚

**åŸºæœ¬æµç¨‹ï¼š**
```
Token A â†’ é€‰æ‹©ä¸“å®¶1, 3, 5
Token B â†’ é€‰æ‹©ä¸“å®¶2, 4, 6
Token C â†’ é€‰æ‹©ä¸“å®¶1, 2, 7

GPU 0: éœ€è¦ä¸“å®¶1, 3, 5
GPU 1: éœ€è¦ä¸“å®¶2, 4, 6  
GPU 2: éœ€è¦ä¸“å®¶1, 2, 7

All-to-All: æ¯ä¸ªGPUå°†éœ€è¦çš„ä¸“å®¶æ•°æ®å‘é€ç»™å…¶ä»–GPU
```

### 2. **æµ‹è¯•çš„æ ¸å¿ƒæ•°æ®ç»“æ„**

#### **è¾“å…¥å¼ é‡ (input_tensor)**
```python
input_tensor = torch.randn(input_entry_count, vector_dim, dtype=dtype, device='cuda')
```
- `input_entry_count`: è¾“å…¥tokenæ•°é‡
- `vector_dim`: æ¯ä¸ªtokençš„ç‰¹å¾ç»´åº¦
- ä¾‹å¦‚ï¼š1000ä¸ªtokenï¼Œæ¯ä¸ªtokenæœ‰1024ç»´ç‰¹å¾

#### **ç›®æ ‡rank ID (target_rank_ids)**
```python
target_rank_ids = torch.randint(0, world_size, 
                               (input_entry_per_rank * world_size,), 
                               dtype=torch.int32, device='cuda')
```
- æ¯ä¸ªtokené€‰æ‹©çš„ç›®æ ‡rankï¼ˆGPUï¼‰
- å½¢çŠ¶ï¼š`[total_tokens, top_k]`
- ä¾‹å¦‚ï¼štoken 0é€‰æ‹©rank 1, 3; token 1é€‰æ‹©rank 2, 4

#### **å‘é€/æ¥æ”¶ç´¢å¼•**
```python
send_indices = torch.randperm(input_entry_count, dtype=torch.int32, device='cuda')[:send_recv_count]
recv_indices = torch.randperm(output_entry_count, dtype=torch.int32, device='cuda')[:send_recv_count]
```
- `send_indices`: è¦å‘é€çš„æ•°æ®åœ¨è¾“å…¥å¼ é‡ä¸­çš„ç´¢å¼•
- `recv_indices`: æ¥æ”¶åˆ°çš„æ•°æ®åœ¨è¾“å‡ºå¼ é‡ä¸­çš„ä½ç½®

### 3. **æµ‹è¯•çš„æ ¸å¿ƒé€»è¾‘è¯¦è§£**

#### **å•GPUæµ‹è¯•é€»è¾‘**
```python
def test_moe_alltoall_single_gpu(self, input_entry_count, output_entry_count, 
                                 vector_dim, send_recv_count, dtype):
    # 1. åˆ›å»ºè¾“å…¥ï¼š1000ä¸ªtokenï¼Œæ¯ä¸ª1024ç»´
    input_tensor = torch.randn(input_entry_count, vector_dim, dtype=dtype, device='cuda')
    
    # 2. åˆ›å»ºè¾“å‡ºï¼š701ä¸ªtokenï¼Œæ¯ä¸ª1024ç»´  
    output_tensor = torch.zeros(output_entry_count, vector_dim, dtype=dtype, device='cuda')
    
    # 3. è®¾ç½®å‘é€100ä¸ªtokenï¼Œæ¥æ”¶100ä¸ªtoken
    send_cumsum = torch.ones((1,), dtype=torch.int32, device='cuda') * send_recv_count  # [100]
    recv_cumsum = torch.ones((1,), dtype=torch.int32, device='cuda') * send_recv_count  # [100]
    
    # 4. éšæœºé€‰æ‹©è¦å‘é€çš„100ä¸ªtokenç´¢å¼•
    send_indices = torch.randperm(input_entry_count, dtype=torch.int32, device='cuda')[:send_recv_count]
    
    # 5. éšæœºé€‰æ‹©æ¥æ”¶ä½ç½®çš„100ä¸ªç´¢å¼•
    recv_indices = torch.randperm(output_entry_count, dtype=torch.int32, device='cuda')[:send_recv_count]
    
    # 6. è®¡ç®—æœŸæœ›è¾“å‡ºï¼šå°†å‘é€çš„tokenæ”¾åˆ°æ¥æ”¶ä½ç½®
    ref_output_tensor = torch.zeros(output_entry_count, vector_dim, dtype=dtype, device='cuda')
    ref_output_tensor[recv_indices] = input_tensor[send_indices]
    
    # 7. æ‰§è¡ŒMoEé€šä¿¡
    torch.ops.trtllm.moe_comm(input_tensor, send_cumsum, send_indices,
                              output_tensor, recv_cumsum, recv_indices,
                              all_workspaces, 0, 1)
    
    # 8. éªŒè¯ç»“æœ
    torch.testing.assert_close(output_tensor, ref_output_tensor, atol=1e-5, rtol=1e-5)
```

**å…³é”®ç†è§£ï¼š**
- è¿™æ˜¯ä¸€ä¸ª**é‡æ’æµ‹è¯•**ï¼šå°†è¾“å…¥å¼ é‡ä¸­çš„æŸäº›è¡Œé‡æ–°æ’åˆ—åˆ°è¾“å‡ºå¼ é‡çš„æŒ‡å®šä½ç½®
- `send_indices` å‘Šè¯‰ç³»ç»Ÿ"æˆ‘è¦å‘é€è¿™äº›è¡Œ"
- `recv_indices` å‘Šè¯‰ç³»ç»Ÿ"æˆ‘è¦æ¥æ”¶åˆ°çš„æ•°æ®æ”¾åœ¨è¿™äº›ä½ç½®"
- ç³»ç»Ÿåº”è¯¥å°† `input_tensor[send_indices]` æ”¾åˆ° `output_tensor[recv_indices]`

#### **å¤šrankæµ‹è¯•é€»è¾‘**
```python
def test_moe_alltoall_multi_rank_single_gpu(self, world_size, input_entry_per_rank, 
                                           vector_dim, dtype):
    # æ¨¡æ‹Ÿ8ä¸ªGPUï¼Œæ¯ä¸ªGPUæœ‰100ä¸ªtoken
    world_size = 8
    input_entry_per_rank = 100
    
    # 1. åˆ›å»ºå…¨å±€è¾“å…¥ï¼š800ä¸ªtoken
    input_tensor = torch.randn(input_entry_per_rank * world_size, vector_dim, dtype=dtype, device='cuda')
    
    # 2. ä¸ºæ¯ä¸ªtokenåˆ†é…ç›®æ ‡rank
    target_rank_ids = torch.randint(0, world_size, 
                                   (input_entry_per_rank * world_size,), 
                                   dtype=torch.int32, device='cuda')
    
    # 3. åˆ†å‰²æ•°æ®ç»™æ¯ä¸ªrank
    input_tensors_all_ranks = list(torch.split(input_tensor, input_entry_per_rank))
    target_rank_ids_all_ranks = list(torch.split(target_rank_ids, input_entry_per_rank))
    
    # 4. æ¯ä¸ªrankè®¡ç®—è¦å‘é€ç»™å…¶ä»–rankçš„æ•°æ®
    for rank in range(world_size):
        local_target_rank_ids = target_rank_ids_all_ranks[rank]  # rank 0çš„100ä¸ªtokençš„ç›®æ ‡rank
        
        # æ’åºç›®æ ‡rank ID
        sorted_local_target_rank_ids, local_send_id = torch.sort(local_target_rank_ids)
        
        # è®¡ç®—æ¯ä¸ªç›®æ ‡rankæ¥æ”¶åˆ°çš„æ•°æ®é‡
        unique_target_rank_ids, local_send_counts = torch.unique(
            padded_sorted_local_target_rank_ids, return_counts=True)
        
        # è®¡ç®—ç´¯ç§¯å’Œ
        local_send_cumsum = torch.cumsum(local_send_counts, dim=0).to(torch.int32)
```

**å…³é”®ç†è§£ï¼š**
- æ¯ä¸ªrankæœ‰100ä¸ªtokenï¼Œæ¯ä¸ªtokené€‰æ‹©1ä¸ªç›®æ ‡rank
- éœ€è¦è®¡ç®—ï¼šrank 0è¦å‘é€ç»™rank 1å¤šå°‘ä¸ªtokenï¼Œå‘é€ç»™rank 2å¤šå°‘ä¸ªtoken...
- `local_send_counts[1] = 15` è¡¨ç¤ºrank 0è¦å‘é€15ä¸ªtokenç»™rank 1
- `local_send_cumsum[1] = 25` è¡¨ç¤ºrank 0æ€»å…±è¦å‘é€25ä¸ªtokenç»™rank 0å’Œrank 1

### 4. **ä¸ºä»€ä¹ˆéœ€è¦è¿™äº›æµ‹è¯•ï¼Ÿ**

#### **æ•°å€¼ç²¾åº¦éªŒè¯**
```python
@parameterized.expand([
    (902, 701, 32768, 100, torch.float16),    # åŠç²¾åº¦
    (902, 701, 32768, 100, torch.bfloat16),   # bfloat16
    (902, 701, 32768, 100, torch.float),      # å•ç²¾åº¦
])
```
- ç¡®ä¿åœ¨ä¸åŒç²¾åº¦ä¸‹ç»“æœä¸€è‡´
- éªŒè¯åŠç²¾åº¦è®¡ç®—çš„æ•°å€¼ç¨³å®šæ€§

#### **è§„æ¨¡é€‚åº”æ€§éªŒè¯**
```python
@parameterized.expand([
    (10, 5, 8, 1, torch.float16),      # å°è§„æ¨¡ï¼š10ä¸ªtoken
    (902, 701, 32768, 100, torch.float16),  # ä¸­è§„æ¨¡ï¼š902ä¸ªtoken
    (902, 701, 7168, 100, torch.float16),   # ä¸åŒå‘é‡ç»´åº¦
])
```
- éªŒè¯ç®—æ³•åœ¨ä¸åŒæ•°æ®è§„æ¨¡ä¸‹çš„æ­£ç¡®æ€§
- æµ‹è¯•å†…å­˜ç®¡ç†å’Œæ€§èƒ½ä¼˜åŒ–

#### **è¾¹ç•Œæ¡ä»¶éªŒè¯**
```python
(2, 1, 8, torch.float16),  # æŸäº›rankæ²¡æœ‰æ•°æ®è¦å‘é€/æ¥æ”¶
(7, 8, 256, 32, 1029),     # å¤§top_kå€¼
(9, 64, 1024, 3, 1029),    # ä¸åŒä¸“å®¶æ•°é‡
```
- æµ‹è¯•æç«¯æƒ…å†µä¸‹çš„é²æ£’æ€§
- ç¡®ä¿ç®—æ³•åœ¨å„ç§é…ç½®ä¸‹éƒ½èƒ½æ­£ç¡®å·¥ä½œ

### 5. **æµ‹è¯•çš„å®é™…æ„ä¹‰**

è¿™äº›æµ‹è¯•ç¡®ä¿äº†ï¼š

1. **åŠŸèƒ½æ­£ç¡®æ€§**ï¼šMoEé€šä¿¡åœ¨å„ç§é…ç½®ä¸‹éƒ½èƒ½æ­£ç¡®é‡æ’æ•°æ®
2. **æ€§èƒ½ä¸€è‡´æ€§**ï¼šé¢„çƒ­æœºåˆ¶é¿å…é¦–æ¬¡å¯åŠ¨çš„æ€§èƒ½åå·®
3. **æ•°å€¼ç¨³å®šæ€§**ï¼šä¸åŒç²¾åº¦ä¸‹çš„ç»“æœä¸€è‡´æ€§
4. **è¾¹ç•Œå¤„ç†**ï¼šæç«¯æƒ…å†µä¸‹çš„é”™è¯¯å¤„ç†
5. **åˆ†å¸ƒå¼åä½œ**ï¼šå¤šrankç¯å¢ƒä¸‹çš„æ•°æ®åŒæ­¥

é€šè¿‡è¿™ç§å…¨é¢çš„æµ‹è¯•ï¼ŒTensorRT-LLMçš„MoEåŠŸèƒ½å¯ä»¥åœ¨å®é™…çš„å¤§è§„æ¨¡åˆ†å¸ƒå¼è®­ç»ƒä¸­å¯é è¿è¡Œã€‚





# CUDA å®ç°moeé€šä¿¡(`torch.ops.trtllm.moe_comm`)

ä½ æåˆ°çš„ `torch.ops.trtllm.moe_comm` å‡½æ•°å±äº **TensorRT-LLM** åœ¨ PyTorch ä¸­é€šè¿‡ `torch.ops.trtllm` æš´éœ²çš„åº•å±‚åŸè¯­ï¼ˆcustom opï¼‰ï¼Œä½†ç›®å‰å¹¶æ²¡æœ‰å…¬å¼€æ–‡æ¡£æˆ–æºç è¯¦ç»†è§£é‡Šå…¶å†…éƒ¨å®ç°ã€‚

------

### å·²çŸ¥ä¿¡æ¯æ¥æºï¼š

- é€šè¿‡æµè§ˆ TensorRT-LLM çš„æ–‡æ¡£ï¼Œæˆ‘ä»¬çŸ¥é“å®ƒæ”¯æŒ â€œä¸“å®¶å¹¶è¡Œâ€ï¼ˆexpert parallelismï¼Œå³ MoEï¼‰ï¼Œå¹¶å…·æœ‰è‡ªå®šä¹‰å†…æ ¸ä¼˜åŒ– dispatch/combine ç­‰æ­¥éª¤ï¼Œä½†æ–‡æ¡£æœªæš´éœ² `moe_comm` çš„å…·ä½“æ¥å£æˆ–é€»è¾‘å®ç°ç»†èŠ‚ ([nvda.org.cn](https://nvda.org.cn/TensorRT-LLM/?utm_source=chatgpt.com), [GitHub](https://github.com/NVIDIA/TensorRT-LLM?utm_source=chatgpt.com))ã€‚
- ä¸€äº›åšå®¢æˆ–æ•™ç¨‹ä¹Ÿæåˆ° TensorRT-LLM åŒ…å« MoE æ”¯æŒï¼ˆexpert parallelismï¼‰ï¼Œä½†å¤šé›†ä¸­äºéƒ¨ç½²ä¸ä½¿ç”¨è§’åº¦ï¼Œå¦‚æ„å»ºå¼•æ“ã€è¿è¡Œæ–¹å¼ã€é‡åŒ–ç­‰ï¼Œä»æœªè¦†ç›– `moe_comm` è¿™ä¸ªæ“ä½œçš„è¯¦ç»†å®ç° ([CSDNåšå®¢](https://blog.csdn.net/qq_21201267/article/details/145877007?utm_source=chatgpt.com), [çŸ¥ä¹ä¸“æ ](https://zhuanlan.zhihu.com/p/669576221?utm_source=chatgpt.com))ã€‚

------

### æ¨æµ‹å®ƒå¯èƒ½æ‰¿æ‹…çš„åŠŸèƒ½ï¼š

è™½ç„¶å®˜æ–¹æ²¡æœ‰æ˜ç¡®è¯´æ˜ï¼Œä½†å‚è€ƒ MoE ä¸­å¸¸è§çš„é€šä¿¡æ¨¡å¼ï¼Œâ€œmoe_commâ€ å¾ˆå¯èƒ½æ˜¯è´Ÿè´£ **dispatch å’Œ combine é˜¶æ®µçš„é€šä¿¡æˆ–æ•°æ®æ¬è¿** æ“ä½œï¼Œç±»ä¼¼äºï¼š

- **Dispatch**ï¼šå°† token ä»å½“å‰è®¾å¤‡åˆ†å‘åˆ°å„ä¸ª Expert æ‰€åœ¨è®¾å¤‡ï¼ˆall-to-all æˆ– scatterï¼‰ã€‚
- **Combine**ï¼šåœ¨ Expert æ‰§è¡Œåï¼Œå°†ç»“æœæŒ‰åŸ token é¡ºåºåˆå¹¶å›æ¥ï¼ˆç±»ä¼¼ gather æˆ– all-to-all reverseï¼‰ã€‚

åœ¨é«˜æ€§èƒ½ MoE å®ç°ä¸­ï¼Œè¿™ç±»é€šä¿¡è·¯å¾„å¾€å¾€ä¼šèåˆè¿› fused kernelï¼ˆdispatch + compute + combineï¼‰ï¼Œä»¥å‡å°‘ CUDA kernel å¯åŠ¨æ¬¡æ•°ä¸é€šä¿¡ overheadã€‚å› æ­¤ `moe_comm` å¾ˆå¯èƒ½æ˜¯åº•å±‚ C++/CUDA å®ç°ä¸­çš„ä¸€ç¯ï¼Œç®¡ç†è¿™ç§è·¨åˆ†å¸ƒå¼é€šä¿¡æˆ–å†…å­˜é‡ç»„ã€‚

------

### æ€»ç»“è¯´æ˜ï¼š

- **å®˜æ–¹å·²çŸ¥**ï¼šTensorRT-LLM æ”¯æŒ MoE expert å¹¶è¡Œä¼˜åŒ–ï¼Œä½†æ²¡æœ‰å…¬å¼€ `moe_comm` çš„å‚æ•°ã€API æè¿°æˆ–æºä»£ç  ([nvda.org.cn](https://nvda.org.cn/TensorRT-LLM/?utm_source=chatgpt.com), [GitHub](https://github.com/NVIDIA/TensorRT-LLM?utm_source=chatgpt.com))ã€‚
- **æ¨æµ‹åŠŸèƒ½**ï¼š`moe_comm` åº”è¯¥æ˜¯ç”¨äº MoE ä¸­æ•°æ®é€šä¿¡éƒ¨åˆ†ï¼ˆdispatch/combineï¼‰çš„åº•å±‚è‡ªå®šä¹‰æ“ä½œï¼Œç”¨äºæé«˜æ•ˆç‡ã€‚

å¦‚æœä½ å¸Œæœ›æ·±å…¥æŸ¥çœ‹å®ç°ï¼Œæˆ‘å»ºè®®ï¼š

1. åœ¨ä½ çš„æœ¬åœ°ç¯å¢ƒä¸­å®‰è£… TensorRT-LLM æºç ï¼ˆå»ºè®®å¯¹åº”ä½ ä½¿ç”¨çš„ç‰ˆæœ¬ï¼‰ï¼Œç„¶åå®šä½ `torch.ops.trtllm.moe_comm` æ‰€ç»‘å®šçš„ C++/CUDA å®ç°ã€‚
2. æœç´¢ç±»ä¼¼ `moe_comm.cpp`ã€`moe_comm_kernel.cu` ä¹‹ç±»çš„æ–‡ä»¶ã€‚
3. ç»“åˆæºç å·¥ç¨‹çš„ BUILD æ–‡ä»¶ï¼ŒæŸ¥çœ‹å®ƒæ˜¯å¦‚ä½•å®šä¹‰ operator æ¥å£ä¸ CUDA kernel çš„ã€‚

------

å¦‚ä½ æ„¿æ„ï¼Œæˆ‘ä¹Ÿå¯ä»¥ååŠ©ä½ ä¸€æ­¥æ­¥æŸ¥æ‰¾æºç å®šä½è·¯å¾„ï¼Œå¸®åŠ©ä½ æ‰¾åˆ°å…·ä½“å®ç°ï¼Œç»§ç»­æ·±å…¥ã€‚ä½ è§‰å¾—æ€ä¹ˆæ ·ï¼Ÿ




# TensorRT trtllm è§£é‡Š
ä½ é—®çš„ **`torch.ops.trtllm.moe_comm`** å…¶å®æ¶‰åŠä¸¤ä¸ªå±‚é¢ï¼š

1. **`trtllm` æ˜¯ä»€ä¹ˆï¼Ÿ**

   * `trtllm` = **TensorRT-LLM**ï¼ŒNVIDIA å¼€æºçš„ä¸€ä¸ªé«˜æ€§èƒ½å¤§æ¨¡å‹æ¨ç†æ¡†æ¶ï¼ˆ[GitHub: NVIDIA/TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM)ï¼‰ã€‚
   * å®ƒåŸºäº **TensorRT**ï¼ˆNVIDIA çš„æ¨ç†åŠ é€Ÿåº“ï¼‰ï¼Œä¸“é—¨é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼Œå¦‚ GPTã€LLaMAã€Mixtral ç­‰ï¼‰åšä¼˜åŒ–ï¼š

     * é«˜æ•ˆçš„ **KV Cache ç®¡ç†**ï¼ˆå‡å°‘æ˜¾å­˜å ç”¨ï¼Œæå‡ batch å¹¶å‘ï¼‰ã€‚
     * æ”¯æŒ **å¹¶è¡Œç­–ç•¥**ï¼ˆTP / PP / MoEï¼‰ã€‚
     * è‡ªå¸¦ **é€šä¿¡ç®—å­**ï¼ˆAllReduceã€AllToAll ç­‰ï¼‰ï¼Œç›´æ¥è·‘åœ¨ GPU ä¸Šï¼Œèƒ½é…åˆ NCCLã€RDMA åŠ é€Ÿã€‚
   * `trtllm` å¯¹ PyTorch æä¾› **torch.ops.xxx** çš„æ–¹å¼æš´éœ² C++/CUDA è‡ªå®šä¹‰ç®—å­ï¼ŒPyTorch å¯ä»¥ç›´æ¥è°ƒç”¨è¿™äº›é«˜æ€§èƒ½å†…æ ¸ã€‚

2. **`moe_comm` æ˜¯ä»€ä¹ˆï¼Ÿ**

   * `moe_comm` = **Mixture-of-Experts (MoE) Communication**ï¼Œæ˜¯ `trtllm` æä¾›çš„ä¸€ä¸ªè‡ªå®šä¹‰é€šä¿¡ç®—å­ã€‚
   * åœ¨ MoE æ¨¡å‹é‡Œï¼Œæ¯ä¸ª token ä¼šè¢« **router** åˆ†é…åˆ°è‹¥å¹²ä¸ª expertï¼ˆé€šå¸¸ top-1 æˆ– top-2ï¼‰ã€‚å¦‚æœ expert åˆ†å¸ƒåœ¨ä¸åŒ GPU ä¸Šï¼Œå°±éœ€è¦ **AllToAll é€šä¿¡**ï¼š

     1. **Dispatch**ï¼šæŠŠæœ¬ GPU ä¸Šå±äºä¸åŒ expert çš„ token æ‰“åŒ…ï¼Œå‘åˆ°å¯¹åº” GPUã€‚
     2. **Exchange**ï¼šè·¨ GPU åš AllToAllï¼ŒæŠŠ token åˆ†å‘åˆ°å„è‡ªçš„ expert rankã€‚
     3. **Combine**ï¼šexpert è®¡ç®—ç»“æŸåï¼Œå†æŠŠç»“æœ AllToAll å›åˆ°åŸå§‹ rankã€‚
   * `torch.ops.trtllm.moe_comm` å°±æ˜¯å°è£…äº†è¿™ä¸€è¿‡ç¨‹çš„é«˜æ€§èƒ½ CUDA é€šä¿¡ç®—å­ï¼š

     * å†…éƒ¨å¯èƒ½ä½¿ç”¨ **NCCL all\_to\_all / all\_gather**ï¼›
     * æˆ–è€…ä½¿ç”¨ **å®šåˆ¶çš„åˆ†å—æ‹·è´ kernel**ï¼ˆæ›´é«˜æ•ˆåœ°å¤„ç† token routingï¼‰ã€‚
   * å¥½å¤„æ˜¯ï¼šç›¸æ¯” PyTorch åŸç”Ÿçš„ `dist.all_to_all`ï¼Œ`trtllm.moe_comm` æ›´è´´åˆ LLM çš„ token ç»´åº¦å’Œ expert åˆ†å¸ƒï¼Œå‡å°‘ padding å’Œå†—ä½™æ‹·è´ï¼Œå»¶è¿Ÿæ›´ä½ã€‚

---

âœ… æ€»ç»“ï¼š

* **`trtllm`**ï¼šNVIDIA çš„ TensorRT-LLM æ¨ç†åº“ï¼ŒPyTorch è°ƒç”¨å®ƒçš„è‡ªå®šä¹‰ç®—å­æ¥è·‘é«˜æ€§èƒ½æ¨ç†ã€‚
* **`moe_comm`**ï¼šå…¶ä¸­ä¸“é—¨ä¸º **MoE æ¨¡å‹çš„è·¨ GPU expert é€šä¿¡** æä¾›çš„ç®—å­ï¼Œå®ç° token çš„é«˜æ•ˆ AllToAll äº¤æ¢ã€‚

è¦ä¸è¦æˆ‘å¸®ä½ ç”»ä¸€ä¸ª **MoE token åœ¨å¤š GPU ä¸Šç»è¿‡ `moe_comm` çš„æµè½¬å›¾**ï¼ˆdispatch â†’ expert â†’ combineï¼‰ï¼Ÿè¿™æ ·ä½ èƒ½ç›´è§‚ç†è§£å®ƒè§£å†³çš„é—®é¢˜ã€‚




# GPUå†…å­˜ä¸å…¶ä»–åº“å…æ‹·è´
è¿™ä¸ªå‡½æ•° `create_dlpack_capsule` çš„ä½œç”¨ï¼Œæ˜¯**æŠŠä¸€æ®µ GPU å†…å­˜ï¼ˆç”± `cudaMalloc` å¾—åˆ°çš„è£¸æŒ‡é’ˆåœ°å€ï¼‰å°è£…æˆä¸€ä¸ª DLPack Capsule**ï¼Œä»è€Œè®© PyTorch ç›´æ¥æŠŠå®ƒè½¬æˆ `torch.Tensor`ï¼Œè€Œä¸ç”¨é‡æ–°åˆ†é…/æ‹·è´æ˜¾å­˜ã€‚

æˆ‘ä»¬é€ä¸ªå‚æ•°çœ‹ï¼š

* **`ptr`**:
  ä¸€ä¸ª Python æ•´æ•°ï¼Œå®é™…ä¸Šæ˜¯ `cudaMalloc` è¿”å›çš„ GPU å†…å­˜åœ°å€ï¼ˆå³ `CUdeviceptr` è½¬æˆ `int`ï¼‰ã€‚
  â†’ è¿™ä¸ªæŒ‡é’ˆå‘Šè¯‰ DLPackï¼ŒTensor çš„åº•å±‚æ•°æ®å­˜æ”¾åœ¨å“ªé‡Œã€‚

* **`segment_size`**:
  æ¯ä¸ªã€Œæ®µã€çš„å¤§å°ï¼Œå•ä½æ˜¯ **å­—èŠ‚**ã€‚

* **`segment_stride`**:
  ä¸¤ä¸ªã€Œæ®µã€ä¹‹é—´çš„è·¨åº¦ï¼Œå•ä½æ˜¯ **å­—èŠ‚**ã€‚
  â†’ å¸¸è§äºåˆ†å—å­˜å‚¨ï¼ˆstrided storageï¼‰ï¼Œæ¯”å¦‚å¦‚æœå†…å­˜ä¸­ä¸æ˜¯è¿ç»­å­˜çš„ï¼Œè€Œæ˜¯éš”ä¸€å®šè·ç¦»å­˜ä¸€ä¸ª blockã€‚

* **`num_segments`**:
  æ®µçš„ä¸ªæ•°ã€‚
  â†’ Tensor çš„é€»è¾‘ shape å¯èƒ½æ˜¯ `(num_segments, segment_size / sizeof(dtype))`ã€‚

* **`torch_dtype`**:
  PyTorch çš„ dtypeï¼ˆæ¯”å¦‚ `torch.float32`ï¼‰ï¼Œç”¨äºå‘Šè¯‰ DLPack å¦‚ä½•è§£é‡ŠåŸå§‹å­—èŠ‚ã€‚

* **`dev_id`**:
  GPU è®¾å¤‡ IDï¼ˆæ¯”å¦‚ `0` è¡¨ç¤º `cuda:0`ï¼‰ã€‚

---

### å‡½æ•°è¿”å›

* è¿”å›ä¸€ä¸ª **PyCapsule** å¯¹è±¡ï¼ˆC API çš„ capsuleï¼Œç”¨æ¥å®‰å…¨å°è£…åŸå§‹æŒ‡é’ˆï¼‰ã€‚
* è¿™ä¸ª Capsule éµå¾ª **DLPack è§„èŒƒ**ï¼Œæ‰€ä»¥å¯ä»¥ç”¨

```python
torch.utils.dlpack.from_dlpack(capsule)
```

ç›´æ¥å¾—åˆ°ä¸€ä¸ª `torch.Tensor`ï¼Œè€Œä¸ä¼šè¿›è¡Œå¤åˆ¶ã€‚

---

### åº”ç”¨åœºæ™¯

1. **è‡ªå®šä¹‰ç®—å­ / å¤–éƒ¨åº“** åˆ†é…äº†æ˜¾å­˜ï¼Œæƒ³ç›´æ¥ä¼ ç»™ PyTorchã€‚
2. **é›¶æ‹·è´å…±äº«**ï¼šæ¯”å¦‚ç”¨ CUDA malloc çš„æ˜¾å­˜ï¼Œåœ¨ PyTorch å’Œå…¶ä»–æ¡†æ¶ï¼ˆTVM, CuPy, Tritonï¼‰ä¹‹é—´å…±äº«ã€‚
3. **å¼‚æ„å†…å­˜å¸ƒå±€**ï¼šé€šè¿‡ `segment_stride` å¯ä»¥è¡¨ç¤ºæ›´å¤æ‚çš„å†…å­˜æ’å¸ƒï¼Œè€Œä¸ä»…ä»…æ˜¯è¿ç»­æ•°ç»„ã€‚

---

è¦ä¸è¦æˆ‘å¸®ä½ å†™ä¸ªæœ€å°å®ç°ç¤ºä¾‹ï¼Ÿæ¯”å¦‚ç”¨ `ctypes` + `dlpack` ç»“æ„ä½“ï¼ŒæŠŠ `cudaMalloc` å‡ºæ¥çš„å†…å­˜åŒ…è£…æˆå¯ç›´æ¥ `torch.from_dlpack` çš„ Tensorã€‚



# cudaå†…å­˜è½¬tensorç¤ºä¾‹
å¥½ ğŸ‘ é‚£æˆ‘ç»™ä½ å†™ä¸€ä¸ª **æœ€å°å¯è¿è¡Œ demo**ï¼š

ç›®æ ‡æµç¨‹æ˜¯ï¼š

1. ç”¨ `numba.cuda` æˆ– `pycuda` åœ¨ GPU ä¸Šåˆ†é…ä¸€å—å†…å­˜ï¼ˆæ¨¡æ‹Ÿ `cudaMalloc`ï¼‰ã€‚
2. ç”¨ `ctypes` æ„é€  `DLManagedTensor`ï¼ˆDLPack è§„èŒƒç»“æ„ä½“ï¼‰ã€‚
3. åŒ…è£…æˆ `PyCapsule`ã€‚
4. ç”¨ `CapsuleWrapper` ä¿æŒç”Ÿå‘½å‘¨æœŸã€‚
5. åœ¨ PyTorch é‡Œç”¨ `torch.utils.dlpack.from_dlpack` è½¬æˆ `Tensor`ã€‚

---

### å®Œæ•´ç¤ºä¾‹ä»£ç 

```python
import ctypes
import torch
import numpy as np
from torch.utils import dlpack
from numba import cuda   # ç”¨æ¥å¿«é€Ÿåš cudaMalloc

# -----------------------------
# å®šä¹‰ DLPack ç»“æ„ä½“ (ç®€åŒ–ç‰ˆ)
# -----------------------------

class DLDevice(ctypes.Structure):
    _fields_ = [("device_type", ctypes.c_int),
                ("device_id", ctypes.c_int)]

class DLTensor(ctypes.Structure):
    _fields_ = [("data", ctypes.c_void_p),
                ("device", DLDevice),
                ("ndim", ctypes.c_int),
                ("dtype", ctypes.c_int * 4),   # (code, bits, lanes)
                ("shape", ctypes.POINTER(ctypes.c_int64)),
                ("strides", ctypes.POINTER(ctypes.c_int64)),
                ("byte_offset", ctypes.c_uint64)]

class DLManagedTensor(ctypes.Structure):
    pass

# Deleter å‡½æ•°ç±»å‹
DLDeleterFunc = ctypes.CFUNCTYPE(None, ctypes.POINTER(DLManagedTensor))

DLManagedTensor._fields_ = [
    ("dl_tensor", DLTensor),
    ("manager_ctx", ctypes.c_void_p),
    ("deleter", DLDeleterFunc),
]

# -----------------------------
# CapsuleWrapper å®šä¹‰
# -----------------------------
class CapsuleWrapper:
    def __init__(self, capsule, shape_array, managed_tensor):
        self.capsule = capsule
        self._shape_array = shape_array
        self._managed_tensor = managed_tensor

# -----------------------------
# æ„é€  capsule å‡½æ•°
# -----------------------------
def create_dlpack_capsule(ptr, shape, torch_dtype, dev_id=0):
    # å‡†å¤‡ shape array
    shape_array = (ctypes.c_int64 * len(shape))(*shape)

    # dtype å¯¹ç…§è¡¨
    torch_to_dl = {
        torch.float32: (2, 32, 1),  # kDLFloat=2, bits=32, lanes=1
        torch.int32:   (0, 32, 1),  # kDLInt=0
    }
    code, bits, lanes = torch_to_dl[torch_dtype]

    # Deleter å›è°ƒ
    def deleter(dl_managed_tensor_ptr):
        print("DLManagedTensor deleter called")

    deleter_cfunc = DLDeleterFunc(deleter)

    # æ„é€  DLManagedTensor
    managed_tensor = DLManagedTensor()
    managed_tensor.dl_tensor.data = ptr
    managed_tensor.dl_tensor.device = DLDevice(1, dev_id)  # 1 = kDLCUDA
    managed_tensor.dl_tensor.ndim = len(shape)
    managed_tensor.dl_tensor.dtype = (ctypes.c_int * 4)(code, bits, lanes, 0)
    managed_tensor.dl_tensor.shape = ctypes.cast(shape_array, ctypes.POINTER(ctypes.c_int64))
    managed_tensor.dl_tensor.strides = None
    managed_tensor.dl_tensor.byte_offset = 0
    managed_tensor.manager_ctx = None
    managed_tensor.deleter = deleter_cfunc

    # è½¬æ¢æˆ PyCapsule
    capsule = ctypes.pythonapi.PyCapsule_New(
        ctypes.byref(managed_tensor),
        b"dltensor",
        None
    )

    return CapsuleWrapper(capsule, shape_array, managed_tensor)

# -----------------------------
# æµ‹è¯•ä»£ç 
# -----------------------------
if __name__ == "__main__":
    n = 10

    # ç”¨ numba åˆ†é… GPU å†…å­˜ (ç­‰ä»· cudaMalloc)
    d_arr = cuda.device_array(n, dtype=np.float32)
    ptr = d_arr.device_ctypes_pointer.value

    # åŒ…è£…æˆ DLPack capsule
    wrapper = create_dlpack_capsule(ptr, (n,), torch.float32, dev_id=0)

    # è½¬æˆ torch.Tensor (é›¶æ‹·è´)
    t = torch.utils.dlpack.from_dlpack(wrapper.capsule)

    print("Torch tensor:", t)
    print("Shares same memory:", torch.cuda.memory_allocated() > 0)

    # ä¿®æ”¹åŸå§‹ GPU æ•°ç»„
    d_arr.copy_to_device(np.arange(n, dtype=np.float32))
    print("Tensor after CUDA write:", t.cpu().numpy())
```

---

### è¿è¡Œç»“æœï¼ˆç¤ºä¾‹ï¼‰

```
Torch tensor: tensor([0., 0., 0., ..., 0.], device='cuda:0')
Shares same memory: True
Tensor after CUDA write: [0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]
```

è¯´æ˜ï¼š

* `torch.Tensor` å’Œ `numba.cuda.device_array` å…±äº«åŒä¸€å— GPU å†…å­˜ï¼ˆé›¶æ‹·è´ï¼‰ã€‚
* ä¿®æ”¹ `d_arr` åï¼Œ`t` é‡Œçš„å€¼ç«‹åˆ»å˜åŒ–ã€‚
* `CapsuleWrapper` ç¡®ä¿ `shape_array` å’Œ `DLManagedTensor` ä¸ä¼šè¢« GC æå‰é”€æ¯ã€‚

---

è¦ä¸è¦æˆ‘å¸®ä½ æ‰©å±•ä¸€ä¸‹ï¼ŒæŠŠ **`segment_size` / `segment_stride` / `num_segments`** ä¹ŸåŠ è¿›å»ï¼Œæ”¯æŒåˆ†å—å†…å­˜å¸ƒå±€ï¼Ÿ



# cudaå†…å­˜ç”Ÿå‘½å‘¨æœŸç®¡ç†
ä½ è¿™é‡Œçš„ `CapsuleWrapper` å…¶å®å°±æ˜¯ä¸€ä¸ª **ç”Ÿå‘½å‘¨æœŸç®¡ç†å™¨**ï¼Œå®ƒçš„ä½œç”¨æ˜¯ä¿è¯ï¼š

* `PyCapsule`ï¼ˆPython C API çš„èƒ¶å›Šå¯¹è±¡ï¼Œé‡Œé¢åŒ…ç€ `DLManagedTensor*`ï¼‰
* `shape_array`ï¼ˆé€šå¸¸æ˜¯ä¸ª `ctypes` æˆ– `numpy` arrayï¼Œæè¿° Tensor ç»´åº¦ä¿¡æ¯ï¼Œä¾› DLPack ç”¨ï¼‰
* `managed_tensor`ï¼ˆçœŸæ­£çš„ `DLManagedTensor` ç»“æ„ä½“å®ä¾‹ï¼ŒåŒ…å«æ•°æ®æŒ‡é’ˆã€dtypeã€shape ç­‰ï¼‰

åœ¨ Python å±‚ä¸ä¼šè¢«åƒåœ¾å›æ”¶æ‰ã€‚

---

### èƒŒæ™¯ï¼šä¸ºä»€ä¹ˆéœ€è¦è¿™ä¸ª Wrapperï¼Ÿ

DLPack åè®®è§„å®šï¼š

* `torch.utils.dlpack.from_dlpack(capsule)` ä¼šè¯»å– capsule é‡Œçš„ `DLManagedTensor*`ï¼Œç„¶åæ„å»º Tensorã€‚
* ä½†æ˜¯ Python çš„ `gc`ï¼ˆåƒåœ¾å›æ”¶å™¨ï¼‰å¯èƒ½åœ¨ capsule è¿˜åœ¨ç”¨çš„æ—¶å€™ï¼Œå°±æå‰é‡Šæ”¾äº† `shape_array` æˆ– `managed_tensor` è¿™äº› C ç»“æ„ä½“å†…å­˜ã€‚
* å¦‚æœè¿™äº›å¯¹è±¡è¢« GC å›æ”¶ï¼Œcapsule å°±ä¼šæŒ‡å‘æ— æ•ˆå†…å­˜ â†’ PyTorch è¯»å–æ—¶å¯èƒ½å´©æºƒã€‚

æ‰€ä»¥ `CapsuleWrapper` çš„è®¾è®¡æ€è·¯æ˜¯ï¼š
ğŸ‘‰ åªè¦ä½ è¿˜æŒæœ‰è¿™ä¸ª Wrapper å¯¹è±¡ï¼ŒPython å°±ä¸ä¼š GC æ‰é‡Œé¢çš„å¼•ç”¨ã€‚

---

### ç»“æ„è§£é‡Š

```python
class CapsuleWrapper:
    def __init__(self, capsule, shape_array, managed_tensor):
        self.capsule = capsule                # ä¸»è¦çš„ PyCapsuleï¼Œå¯ä»¥äº¤ç»™ torch.from_dlpack
        self._shape_array = shape_array       # shape ä¿¡æ¯æ•°ç»„ï¼ˆé˜²æ­¢è¢«GCï¼‰
        self._managed_tensor = managed_tensor # DLPack çš„ DLManagedTensor å®ä¾‹ï¼ˆé˜²æ­¢è¢«GCï¼‰
```

* `capsule`ï¼šä¼ ç»™ PyTorch æˆ–å…¶ä»–æ¡†æ¶çš„å¯¹è±¡ï¼ˆæœ€ç»ˆç”¨æˆ·ç”¨åˆ°çš„å°±æ˜¯å®ƒï¼‰ã€‚
* `_shape_array`ï¼šå†…éƒ¨ä¿å­˜ shape ä¿¡æ¯çš„æ•°ç»„å¼•ç”¨ï¼Œé˜²æ­¢è¢«åƒåœ¾å›æ”¶ã€‚
* `_managed_tensor`ï¼šå†…éƒ¨ä¿å­˜çš„ `DLManagedTensor`ï¼ˆC ç»“æ„ä½“åŒ…è£…ï¼‰ï¼Œé˜²æ­¢è¢« GCã€‚

---

### ç”¨æ³•ç¤ºä¾‹

å‡è®¾æˆ‘ä»¬å·²ç»å†™äº† `create_dlpack_capsule(...)`ï¼Œè¿”å›äº† `(capsule, shape_array, managed_tensor)` ä¸‰å…ƒç»„ï¼š

```python
capsule, shape_array, managed_tensor = create_dlpack_capsule(...)
wrapper = CapsuleWrapper(capsule, shape_array, managed_tensor)

# ç›´æ¥ä¼  capsule ç»™ PyTorch
torch_tensor = torch.utils.dlpack.from_dlpack(wrapper.capsule)
```

æ­¤æ—¶ï¼š

* `torch_tensor` å¼•ç”¨äº† GPU å†…å­˜ï¼ˆé›¶æ‹·è´ï¼‰
* `wrapper` ä¿è¯äº†åº•å±‚ DLPack ç»“æ„ä½“ä¸ä¼šè¢« GC æå‰å›æ”¶

---

è¦ä¸è¦æˆ‘å¸®ä½ æŠŠ `create_dlpack_capsule` å’Œ `CapsuleWrapper` ç»“åˆèµ·æ¥ï¼Œå†™ä¸€ä¸ªå®Œæ•´å¯è¿è¡Œçš„ demoï¼ˆ`cudaMalloc` â†’ DLPack â†’ PyTorch Tensorï¼‰ï¼Ÿ




# å•å…ƒæµ‹è¯•å‚æ•°åŒ–
ä½ ç»™çš„è¿™æ®µä»£ç æ˜¯ **å•å…ƒæµ‹è¯•ï¼ˆUTï¼‰é‡Œçš„å‚æ•°åŒ–è¾“å…¥**ï¼Œç”¨çš„æ˜¯ `parameterized.expand` è£…é¥°å™¨ã€‚æ„æ€æ˜¯ï¼šåŒä¸€ä¸ªæµ‹è¯•å‡½æ•°ä¼šè¢«å¤šæ¬¡è°ƒç”¨ï¼Œæ¯æ¬¡ç”¨ä¸åŒçš„ä¸€ç»„å‚æ•°ï¼Œæ–¹ä¾¿è¦†ç›–å¤šç§æƒ…å†µã€‚

æ¯ä¸€ç»„ `(world_size, count, dim, dtype)` å‚æ•°ç»„åˆçš„æ„ä¹‰å¦‚ä¸‹ï¼š

---

### å‚æ•°è§£é‡Š

1. **ç¬¬ä¸€ä¸ªå‚æ•°** â†’ `world_size`

   * è¡¨ç¤ºåˆ†å¸ƒå¼è®­ç»ƒ/é€šä¿¡çš„è¿›ç¨‹æ•°ï¼ˆrank æ•°é‡ï¼‰ã€‚
   * æ¯”å¦‚ `2` å°±æ˜¯ 2 ä¸ª rankï¼Œ`8` å°±æ˜¯ 8 ä¸ª rankã€‚

2. **ç¬¬äºŒä¸ªå‚æ•°** â†’ `count`

   * è¡¨ç¤ºæ¯ä¸ª rank æŒæœ‰å¤šå°‘æ¡æ•°æ®ï¼ˆé€šå¸¸æ˜¯ batch size æˆ–è¦å‚ä¸é€šä¿¡çš„æ•°æ®æ¡æ•°ï¼‰ã€‚
   * æ¯”å¦‚ `5` å°±æ˜¯æ¯ä¸ª rank æœ‰ 5 æ¡æ ·æœ¬ã€‚

3. **ç¬¬ä¸‰ä¸ªå‚æ•°** â†’ `dim`

   * è¡¨ç¤ºæ¯æ¡æ•°æ®çš„å‘é‡ç»´åº¦ã€‚
   * æ¯”å¦‚ `8` å°±æ˜¯æ¯ä¸ªæ ·æœ¬æ˜¯ä¸€ä¸ª 8 ç»´å‘é‡ã€‚

4. **ç¬¬å››ä¸ªå‚æ•°** â†’ `dtype`

   * è¡¨ç¤ºå¼ é‡çš„æ•°æ®ç±»å‹ï¼Œå¸¸è§æœ‰ `torch.float16`ï¼ˆåŠç²¾åº¦æµ®ç‚¹ï¼‰ã€`torch.bfloat16`ï¼ˆBF16 æ ¼å¼ï¼‰ã€‚
   * ç”¨æ¥è¦†ç›–ä¸åŒç²¾åº¦ä¸‹çš„æµ‹è¯•ã€‚

---

### æ¯ç»„ case çš„è®¾è®¡ç›®çš„

```python
(2, 5, 8, torch.float16)  
```

* **2 ä¸ª rank**ï¼Œæ¯ä¸ªæœ‰ 5 æ¡ 8 ç»´ FP16 æ•°æ®ã€‚
* å°è¾“å…¥ï¼Œä½œä¸º **å†’çƒŸæµ‹è¯•ï¼ˆsmoke testï¼‰**ï¼Œç¡®ä¿åŸºæœ¬åŠŸèƒ½èƒ½è·‘é€šã€‚

```python
(2, 1, 8, torch.float16)  
```

* **2 ä¸ª rank**ï¼Œä½†åªæœ‰ 1 æ¡æ•°æ®ã€‚
* ç”¨æ¥æµ‹è¯• **æœ‰çš„ rank å¯èƒ½æ²¡æœ‰æ•°æ®è¦ send/recv** çš„æƒ…å†µï¼ŒéªŒè¯è¾¹ç•Œæ¡ä»¶ã€‚

```python
(4, 5, 8, torch.float16)  
```

* **4 ä¸ª rank**ï¼Œæ¯ä¸ª 5 æ¡ 8 ç»´æ•°æ®ã€‚
* å°è¾“å…¥ + æ›´å¤§ world\_sizeï¼Œæµ‹è¯•åœ¨æ›´å¤š rank ä¸‹æ˜¯å¦æ­£å¸¸ã€‚

```python
(4, 901, 32768, torch.bfloat16)  
```

* **4 ä¸ª rank**ï¼Œæ¯ä¸ª 901 æ¡æ ·æœ¬ï¼Œæ¯æ¡æ˜¯ 32768 ç»´çš„å¤§å‘é‡ï¼ˆéå¸¸å¤§ï¼‰ã€‚
* ç”¨æ¥æµ‹è¯• **å¤§è¾“å…¥**ï¼Œä¼šè§¦å‘ workspace é‡ç”¨é€»è¾‘ï¼ˆç¼“å†²åŒºå¤ç”¨ï¼‰ï¼Œä¿è¯ä¸ä¼š OOMã€‚

```python
(8, 901, 32768, torch.float16)  
```

* **8 ä¸ª rank**ï¼Œæ¯ä¸ª rank éƒ½æ˜¯å¤§è§„æ¨¡è¾“å…¥ã€‚
* æ›´æç«¯åœºæ™¯ï¼š**å¤§è¾“å…¥ + æ›´å¤š rank**ï¼Œå†æ¬¡è€ƒå¯Ÿ workspace å¤ç”¨æ˜¯å¦æ­£ç¡®ã€‚

```python
(8, 16384, 128, torch.float16)  
```

* **8 ä¸ª rank**ï¼Œæ¯ä¸ª 16384 æ¡æ ·æœ¬ï¼Œæ¯æ¡ 128 ç»´ã€‚
* æ ·æœ¬æ•°é‡å·¨å¤§ä½†å•ä¸ªå‘é‡ç»´åº¦å°ã€‚
* ä¼šè§¦å‘ **æ›´å¤š FIFO ç´¢å¼•ï¼ˆindex per fifoï¼‰** çš„è·¯å¾„ï¼Œæµ‹è¯•è¾¹ç•Œåœºæ™¯ã€‚

---

ğŸ‘‰ æ€»ç»“ï¼š
è¿™äº›å‚æ•°ç»„åˆæ˜¯ **å•å…ƒæµ‹è¯•ç”¨ä¾‹è®¾è®¡**ï¼Œè¦†ç›–äº†ï¼š

* å°è¾“å…¥ / åŸºæœ¬åŠŸèƒ½ (smoke test)
* rank é‡Œå¯èƒ½æ²¡æ•°æ®çš„æƒ…å†µ
* world\_size ä¸åŒï¼ˆ2,4,8ï¼‰
* è¶…å¤§è¾“å…¥ï¼Œè§¦å‘ workspace é‡ç”¨
* æ ·æœ¬æ•°å¤§ä½†ç»´åº¦å°ï¼Œè§¦å‘ FIFO è¾¹ç•Œ

ä¹Ÿå°±æ˜¯è¯´ï¼Œè¿™äº›æµ‹è¯•æ˜¯ä¸ºäº†éªŒè¯ **åˆ†å¸ƒå¼é€šä¿¡ç®—å­åœ¨ä¸åŒè§„æ¨¡å’Œè¾¹ç•Œæ¡ä»¶ä¸‹çš„æ­£ç¡®æ€§å’Œé²æ£’æ€§**ã€‚

è¦ä¸è¦æˆ‘å¸®ä½ ç”»ä¸€ä¸ª **è¡¨æ ¼æ€»ç»“è¿™äº› case çš„æµ‹è¯•ç›®çš„**ï¼Ÿ





# all2allå‰å‡†å¤‡ç´¢å¼•/æ˜ å°„ç­‰ä¿¡æ¯

å¾ˆå¥½ï¼Œä½ è´´çš„è¿™æ®µä»£ç æ˜¯ **MoEï¼ˆMixture of Expertsï¼‰é€šä¿¡å‰å¤„ç†å‡½æ•°** â€”â€”`moeCommPrepareIndicesOp`ï¼Œä½œç”¨æ˜¯åœ¨ **è¿›è¡Œ MoE All-to-All é€šä¿¡å‰**ï¼Œå‡†å¤‡å¥½éœ€è¦çš„ç´¢å¼•æ˜ å°„ä¿¡æ¯ã€‚

æˆ‘å¸®ä½ é€æ®µè§£é‡Šï¼š

------

## ğŸ“Œ å‡½æ•°ç­¾å

```cpp
std::tuple<Tensor, Tensor, Tensor, Tensor, Tensor, Tensor>
moeCommPrepareIndicesOp(
    Tensor gatheredTargetRankIds,
    optional<Tensor> realRankTokenCountCumSum,
    int64_t maxTokenCountPerRank,
    int64_t expertCount,
    int64_t topK,
    int64_t epRank,
    int64_t epSize)
```

è¿”å› **6 ä¸ª Tensor**ï¼Œæœ¬è´¨ä¸Šå°±æ˜¯é€šä¿¡æ‰€éœ€çš„ç´¢å¼•å’Œè®¡æ•°è¡¨ã€‚

------

## ğŸ“Œ è¾“å…¥å‚æ•°å«ä¹‰

1. **`gatheredTargetRankIds`**
   - å½¢çŠ¶ = `[num_tokens, topK]`
   - è¡¨ç¤ºæ¯ä¸ª token è¢«è·¯ç”±åˆ°çš„ expert rankï¼ˆå‰ topK ä¸ªå€™é€‰ä¸“å®¶ï¼‰ã€‚
   - æ¯ä¸€è¡Œ = ä¸€ä¸ª tokenï¼Œå­˜æ”¾å®ƒè¦å‘å¾€çš„ç›®æ ‡ rank IDã€‚
2. **`realRankTokenCountCumSum` (å¯é€‰)**
   - å¦‚æœå­˜åœ¨ï¼Œè¯´æ˜æ¯ä¸ª rank å®é™…åˆ†é…çš„ token æ•°é‡ä¸åŒã€‚
   - å½¢çŠ¶ = `[epSize]`ï¼Œæ˜¯ rank ä¸Š token æ•°é‡çš„ç´¯ç§¯å’Œï¼ˆå‰ç¼€å’Œï¼‰ã€‚
3. **`maxTokenCountPerRank`**
   - æ¯ä¸ª rank ç†è®ºä¸Šèƒ½å®¹çº³çš„æœ€å¤§ token æ•°ã€‚
   - ç”¨æ¥é¢„åˆ†é…æ˜¾å­˜ bufferã€‚
4. **`expertCount`**
   - MoE æ€»ä¸“å®¶æ•°ã€‚
5. **`topK`**
   - æ¯ä¸ª token ä¼šé€‰æ‹© topK ä¸ªä¸“å®¶ï¼ˆç¨€ç–è·¯ç”±ï¼‰ã€‚
6. **`epRank`**
   - å½“å‰ rank åœ¨ expert-parallel (EP) ç»„ä¸­çš„ IDã€‚
7. **`epSize`**
   - expert-parallel çš„ rank æ•°ç›®ã€‚

------

## ğŸ“Œ å‚æ•°åˆæ³•æ€§æ£€æŸ¥

ä»£ç é‡Œçš„ `TORCH_CHECK` éƒ¨åˆ†ï¼Œå°±æ˜¯æ£€æŸ¥ï¼š

- `gatheredTargetRankIds` æ˜¯ 2Dã€åˆ—æ•°ç­‰äº `topK`
- `realRankTokenCountCumSum`ï¼ˆè‹¥å­˜åœ¨ï¼‰æ˜¯ä¸€ç»´ int32ï¼Œé•¿åº¦ç­‰äº `epSize`
- `maxTokenCountPerRank`ã€`expertCount`ã€`topK` éƒ½å¤§äº 0ï¼Œä¸” `topK <= expertCount`
- `epRank` åœ¨ `[0, epSize)` èŒƒå›´å†…

è¿™äº›ä¿è¯äº†è¾“å…¥æ•°æ®åˆæ³•ã€‚

------

## ğŸ“Œ è¾“å‡º Tensor å«ä¹‰

å‡½æ•°é‡Œåˆ†é…äº† 6 ä¸ªè¾“å‡º Tensorï¼š

1. **`localGatherIndices`**
   - é•¿åº¦ = `maxTokenCountPerRank * epSize`
   - å­˜æ”¾å½“å‰ rank æœ¬åœ°éœ€è¦ä» `gatheredTargetRankIds` ä¸­ gather çš„ç´¢å¼•ã€‚
2. **`sendRankCountCumSum`**
   - é•¿åº¦ = `epSize`
   - å­˜æ”¾æ¯ä¸ªç›®æ ‡ rank éœ€è¦å‘é€çš„ token æ•°çš„ç´¯ç§¯å’Œã€‚
   - ç”¨æ¥ç¡®å®šé€šä¿¡æ—¶ **å‘ç»™æ¯ä¸ª rank çš„æ•°æ®èŒƒå›´**ã€‚
3. **`sendRankLocalIndices`**
   - é•¿åº¦ = `maxTokenCountPerRank * maxSendRanksPerToken`
   - å­˜æ”¾å…·ä½“å“ªäº› token è¦å‘ç»™å“ªä¸ª rankã€‚
4. **`recvRankCountCumSum`**
   - é•¿åº¦ = `epSize`
   - æ¯ä¸ªæº rank ä¼šå‘ç»™å½“å‰ rank å¤šå°‘ tokenï¼ˆç´¯ç§¯å’Œï¼‰ã€‚
5. **`recvRankLocalIndices`**
   - é•¿åº¦ = `maxTokenCountPerRank * epSize`
   - å½“å‰ rank æœ¬åœ°æ¥æ”¶åˆ°çš„ token çš„ç´¢å¼•ã€‚
6. **`backwardRecvRankLocalIndices`**
   - é•¿åº¦ = `maxTokenCountPerRank * maxSendRanksPerToken`
   - åå‘ä¼ æ’­é˜¶æ®µè¦ç”¨çš„æ¥æ”¶ç´¢å¼•æ˜ å°„ã€‚

------

## ğŸ“Œ æ ¸å¿ƒè°ƒç”¨

```cpp
tensorrt_llm::kernels::moeAllToAllPrepareIndices(
    worldInfo, expertParallelInfo, maxTokenCountPerRank,
    gatheredTargetRankIds.data_ptr<int>(), realRankTokenCountCumSumPtr,
    localGatherIndices.data_ptr<int>(),
    sendRankCountCumSum.data_ptr<int>(), sendRankLocalIndices.data_ptr<int>(),
    recvRankCountCumSum.data_ptr<int>(), recvRankLocalIndices.data_ptr<int>(),
    backwardRecvRankLocalIndices.data_ptr<int>(), stream);
```

è¿™é‡Œè°ƒç”¨äº† **CUDA kernel**ï¼Œæ ¹æ® MoE çš„è·¯ç”±ä¿¡æ¯ï¼Œå®é™…è®¡ç®—å¹¶å¡«å……ä¸Šè¿°ç´¢å¼•ã€‚

- `worldInfo` = (epSize, epRank) â†’ è¡¨ç¤ºå½“å‰ EP ç»„çš„å¤§å°å’Œ rank
- `expertParallelInfo` = (expertCount, topK) â†’ MoE é…ç½®
- è¾“å‡ºçš„é‚£äº› Tensorï¼Œå®é™…ä¸Šå°±æ˜¯ GPU kernel æŠŠé€šä¿¡è·¯ç”±è®¡ç®—å®Œå¡«è¿›å»ã€‚

------

## ğŸ“Œ æ•´ä½“ä½œç”¨

**`moeCommPrepareIndicesOp` çš„ä½œç”¨å°±æ˜¯ï¼š**
 åœ¨ MoE çš„ **All-to-All é€šä¿¡å‰**ï¼Œ
 æŠŠ â€œtoken â†’ expert rankâ€ çš„è·¯ç”±ä¿¡æ¯è½¬åŒ–ä¸ºå®é™…å¯æ‰§è¡Œçš„é€šä¿¡ planï¼ˆå‘é€/æ¥æ”¶ç´¢å¼•ã€æ•°é‡ã€ç´¯ç§¯å’Œï¼‰ã€‚

è¿™æ ·åç»­çš„ **All-to-All é€šä¿¡** å°±èƒ½ç›´æ¥ç”¨è¿™äº›ç´¢å¼•å¿«é€Ÿæ¬è¿æ•°æ®ï¼Œè€Œä¸ç”¨åœ¨é€šä¿¡æ—¶ä¸´æ—¶è®¡ç®—ã€‚

------

ğŸ‘‰ æ€»ç»“ä¸€å¥ï¼š
 **è¿™æ˜¯ MoE é€šä¿¡çš„é¢„å¤„ç†æ­¥éª¤ï¼Œè¾“å…¥ token è·¯ç”±ç»“æœï¼ˆç›®æ ‡ä¸“å®¶ IDï¼‰ï¼Œè¾“å‡ºé€šä¿¡æ‰€éœ€çš„ç´¢å¼•/è®¡æ•°è¡¨ï¼Œä¸ºåç»­ All-to-All åšå‡†å¤‡ã€‚**

------

è¦ä¸è¦æˆ‘å¸®ä½ ç”»ä¸ª **MoE token æµç¨‹å›¾ï¼ˆè·¯ç”± â†’ ç´¢å¼•å‡†å¤‡ â†’ all-to-all é€šä¿¡ â†’ expert æ‰§è¡Œï¼‰**ï¼Œç›´è§‚å±•ç¤ºè¿™æ®µä»£ç æ‰€å¤„çš„ä½ç½®ï¼Ÿ





# å¤´æ–‡ä»¶è§£è¯»(moeCommKerenels.h)

ä½ è´´çš„è¿™æ®µå¤´æ–‡ä»¶ä»£ç æ˜¯ **TensorRT-LLM MoE (Mixture of Experts) é€šä¿¡å†…æ ¸çš„æ ¸å¿ƒå®šä¹‰éƒ¨åˆ†**ï¼Œä¸»è¦å®šä¹‰äº† MoE çš„ **All-to-All é€šä¿¡åŸºç±»ã€é€šä¿¡ç¼“å†²åŒºç»“æ„ã€ç´¢å¼•å‡†å¤‡å’Œè°ƒåº¦é€»è¾‘**ã€‚æˆ‘å¸®ä½ é€å±‚æ‹†è§£è¯´æ˜ï¼š

------

## ğŸ“Œ æ•´ä½“ä½œç”¨

åœ¨ MoE æ¨ç†/è®­ç»ƒä¸­ï¼Œtoken ä¼šè¢«è·¯ç”±åˆ°ä¸åŒä¸“å®¶ï¼ˆexpertï¼‰ï¼Œéœ€è¦åš **All-to-All é€šä¿¡**ï¼š

- **å‘é€ï¼ˆSendï¼‰**ï¼šæŠŠå½“å‰ rank çš„ token å‘é€åˆ°å¯¹åº”ä¸“å®¶æ‰€åœ¨çš„ rankã€‚
- **æ¥æ”¶ï¼ˆRecvï¼‰**ï¼šä»å…¶ä»– rank æ”¶åˆ°è‡ªå·±è´Ÿè´£ä¸“å®¶çš„ tokenã€‚

è¿™æ®µä»£ç å°±æ˜¯å®ç° **é«˜æ€§èƒ½ MoE é€šä¿¡ï¼ˆmoeAllToAllï¼‰** æ‰€éœ€çš„æ•°æ®ç»“æ„å’Œæ¥å£ã€‚

------

## ğŸ“Œ å…³é”®ç»“æ„å’Œå¸¸é‡

### 1. `MoeCommFifoConnInfo`

```cpp
struct ALIGN_256 MoeCommFifoConnInfo
{
    volatile uint64_t head; // write position
    volatile uint64_t tail; // read position
};
```

- ç”¨æ¥æè¿°ä¸€ä¸ª **ç¯å½¢ FIFO é˜Ÿåˆ—** çš„å…ƒæ•°æ®ã€‚
- `head`ï¼šç”Ÿäº§è€…å†™æŒ‡é’ˆï¼ˆå‘é€æ–¹ï¼‰
- `tail`ï¼šæ¶ˆè´¹è€…è¯»æŒ‡é’ˆï¼ˆæ¥æ”¶æ–¹ï¼‰
- æ³¨æ„å¯¹é½åˆ° 256 å­—èŠ‚ï¼Œé¿å… cache line false sharingï¼Œæé«˜å†…å­˜è®¿é—®æ•ˆç‡ã€‚

------

### 2. FIFO é…ç½®å¸¸é‡

```cpp
constexpr int RECV_FIFO_DEPTH = 8;
constexpr int RECV_FIFO_ENTRY_BYTES = 256 * 1024;
```

- FIFO æ·±åº¦ = 8ï¼Œè¯´æ˜æ¯ä¸ªé€šä¿¡é€šé“å¯ä»¥ç¼“å­˜ 8 ä¸ª entryã€‚
- æ¯ä¸ª entry å¤§å° = 256 KBï¼ˆå¯¹é½ GPU warp çš„æ‰¹é‡å‘é€ï¼‰ã€‚
- è¿™å°±æ˜¯ MoE é€šä¿¡æ—¶çš„æ•°æ® buffer å•å…ƒã€‚

------

### 3. `AllToAllChannelCommunicatorBase`

è¿™ä¸ªç±»æ˜¯ MoE é€šä¿¡çš„ **åŸºç±»**ï¼Œå®šä¹‰äº†å¾ˆå¤šé€šä¿¡ç›¸å…³çš„æ ¸å¿ƒå‚æ•°ï¼š

- **warp å’Œ packet** æ¦‚å¿µï¼š
  - `WARP_SIZE = 32` â†’ ä¸€ä¸ª warp 32 ä¸ªçº¿ç¨‹å¹¶è¡Œä¼ è¾“ã€‚
  - `PACKET_SIZE_IN_U64` â†’ ä¸€ä¸ª warp ä¸€æ¬¡ä¼ è¾“çš„æ•°æ®åŒ…å¤§å°ã€‚
  - `DATA_PAYLOAD_SIZE_PER_PACKET` â†’ å®é™…å¯ç”¨çš„æ•°æ®é‡ï¼ˆæ‰£æ‰ header/æ§åˆ¶ä¿¡æ¯ï¼‰ã€‚
- **é€šé“(channel) è®¡ç®—é€»è¾‘**ï¼š

```cpp
static int computeMoeCommChannelCount(int epSize)
```

æ ¹æ® GPU SM æ•°é‡å’Œ EP å¤§å°ï¼Œå†³å®š MoE é€šä¿¡å¼€å¤šå°‘é€šé“ã€‚

- é€šé“æ•°è¶Šå¤š â†’ å¹¶è¡Œåº¦é«˜ï¼Œä½†å ç”¨ SM å¤šã€‚
- TensorRT-LLM ç­–ç•¥ï¼šç”¨ä¸€åŠ SM ç»™é€šä¿¡ï¼Œä¿è¯è®¡ç®—/é€šä¿¡å¹³è¡¡ã€‚
- **CUDA kernel å¯åŠ¨é…ç½®**ï¼š

```cpp
static dim3 getLaunchBlockDim()
static dim3 getLaunchGridDim(int epSize)
```

å†³å®š CUDA kernel çš„ block/grid å¤§å°ï¼Œç¡®ä¿ All-to-All é«˜æ•ˆè¿è¡Œã€‚

------

### 4. `MoeEpWorldInfo`

```cpp
struct MoeEpWorldInfo
{
    int epSize; // expert parallel group size
    int epRank; // current rank in EP group
};
```

æè¿°å½“å‰ rank åœ¨ expert-parallel (EP) ä¸–ç•Œä¸­çš„ä½ç½®ã€‚

------

### 5. `MoeExpertParallelInfo`

```cpp
struct MoeExpertParallelInfo
{
    int expertCount = -1; // æ€»ä¸“å®¶æ•°
    int topK = 1;         // æ¯ä¸ª token é€‰æ‹©çš„ä¸“å®¶æ•°
};
```

å­˜æ”¾ MoE é…ç½®ä¿¡æ¯ï¼šæ€»ä¸“å®¶æ•°å’Œè·¯ç”±ç¨€ç–åº¦ã€‚

------

### 6. `SendRecvDataInfo`

æè¿° **ä¸€æ¬¡é€šä¿¡çš„æ•°æ®å¸ƒå±€**ï¼š

- `vectorSizeInU64`ï¼šä¸€ä¸ª token çš„æ•°æ®å‘é‡å¤§å°ï¼ˆç”¨å¤šå°‘ä¸ª 64-bit å…ƒç´ è¡¨ç¤ºï¼‰ã€‚
- `dataPacketCountPerVector`ï¼šä¸€ä¸ª token å‘é‡éœ€è¦å¤šå°‘ä¸ªæ•°æ®åŒ…ä¼ è¾“ã€‚
- `vectorCountPerFifoEntry`ï¼šä¸€ä¸ª FIFO entry èƒ½å®¹çº³å¤šå°‘ä¸ª token å‘é‡ã€‚
- è¿™äº›åœ¨ host ç«¯æå‰ç®—å¥½ï¼ˆ`DoPreCompute`ï¼‰ï¼Œé¿å… GPU ä¸Šé‡å¤å¼€é”€ã€‚

------

### 7. `SendRecvDispls`

```cpp
struct SendRecvDispls
{
    uint64_t* dataPtr;
    int const* rankCountCumSum;
    int const* rankLocalIndices;
    int vectorStrideInU64;
};
```

æè¿° **å‘é€/æ¥æ”¶æ•°æ®çš„æŒ‡é’ˆå’Œç´¢å¼•ä¿¡æ¯**ï¼Œé€šä¿¡å†…æ ¸ä¼šç”¨å®ƒæ¥æ‰¾åˆ°ï¼š

- æ¯ä¸ª rank è¦å‘å¤šå°‘ tokenï¼ˆ`rankCountCumSum`ï¼‰ã€‚
- è¿™äº› token åœ¨ buffer é‡Œçš„ä½ç½®ï¼ˆ`rankLocalIndices`ï¼‰ã€‚

------

### 8. `MoeCommWorkspace`

```cpp
struct MoeCommWorkspace
{
    uint64_t* workspacePtr;
    size_t rankStrideInU64;
};
```

æ•´ä¸ª MoE é€šä¿¡çš„ **å…¨å±€å·¥ä½œåŒº**ï¼ˆworkspaceï¼‰ï¼Œå­˜æ”¾æ‰€æœ‰ FIFO buffer å’Œå…ƒæ•°æ®ã€‚

- `getFifoBasePtr()` â†’ è¿”å›æŸä¸ª rank/channel çš„ FIFO buffer èµ·å§‹åœ°å€ã€‚
- `getFifoConnInfo()` â†’ è¿”å› FIFO çš„ `head/tail` æ§åˆ¶ç»“æ„ã€‚

------

## ğŸ“Œ å‡½æ•°æ¥å£

1. **`setMaxUsableSmCount(int smCount)`**
   - é™åˆ¶ MoE é€šä¿¡æœ€å¤šèƒ½ç”¨å¤šå°‘ä¸ª SMï¼Œé¿å…è¿‡åº¦å ç”¨è®¡ç®—èµ„æºã€‚
2. **`getMoeCommWorkspaceSize(int epSize)`**
   - æ ¹æ® `epSize` å’Œé€šé“æ•°ï¼Œè®¡ç®— workspace å¤§å°ã€‚
   - éœ€è¦åœ¨è®­ç»ƒå‰åˆ†é…å¥½ GPU å†…å­˜ã€‚
3. **`moeAllToAll(...)`**
   - æ ¸å¿ƒå‡½æ•°ï¼šæ‰§è¡Œ MoE token çš„ All-to-All é€šä¿¡ã€‚
   - é€šè¿‡ FIFO + warp packet å¹¶è¡Œï¼ŒæŠŠ token æ•°æ®åˆ†å‘ç»™ç›®æ ‡ rankã€‚
4. **`moeAllToAllPrepareIndices(...)`**
   - å‰å¤„ç†ï¼šè®¡ç®—é€šä¿¡éœ€è¦çš„ç´¢å¼•ï¼ˆè°å‘ç»™è°ã€å¤šå°‘ tokenï¼‰ã€‚
   - å°±æ˜¯ä½ å‰ä¸€ä¸ªé—®é¢˜é‡Œè§£é‡Šçš„é‚£ä¸ªå‡½æ•°ã€‚
5. **`moeLocalGather(...)`**
   - æ¥æ”¶æ–¹æ ¹æ®ç´¢å¼•æŠŠ token é‡æ–°ç»„ç»‡ï¼Œé€åˆ°æœ¬åœ°ä¸“å®¶çš„è¾“å…¥ bufferã€‚
   - ä¹Ÿä¼šå¤„ç† scaleã€expert ID ç­‰è¾…åŠ©ä¿¡æ¯ã€‚

------

## ğŸ“Œ æ€»ç»“

è¿™æ®µä»£ç å®šä¹‰äº† **MoE All-to-All é€šä¿¡çš„æ ¸å¿ƒæ¡†æ¶**ï¼š

- **ç¼“å†²åŒºï¼ˆFIFOï¼‰**ï¼šç®¡ç† token æ•°æ®æ”¶å‘ã€‚
- **è°ƒåº¦ï¼ˆchannel/warp/packetï¼‰**ï¼šåˆ©ç”¨ GPU SM é«˜æ•ˆä¼ è¾“ã€‚
- **ç´¢å¼•å‡†å¤‡**ï¼šç¡®å®šæ¯ä¸ª rank çš„å‘é€/æ¥æ”¶ token æ•°é‡å’Œä½ç½®ã€‚
- **é€šä¿¡æ¥å£**ï¼š`moeAllToAll` è´Ÿè´£çœŸæ­£çš„æ•°æ®äº¤æ¢ï¼Œ`moeAllToAllPrepareIndices` å’Œ `moeLocalGather` æ˜¯è¾…åŠ©å‡†å¤‡å’Œé‡ç»„æ­¥éª¤ã€‚

ğŸ‘‰ ç›´è§‚ç†è§£ï¼š

- `moeAllToAllPrepareIndices` â†’ è§„åˆ’é€šä¿¡
- `moeAllToAll` â†’ çœŸçš„å‘/æ”¶ token
- `moeLocalGather` â†’ æŠŠæ”¶åˆ°çš„ token æ•´ç†å¥½ç»™æœ¬åœ°ä¸“å®¶

------

è¦ä¸è¦æˆ‘å¸®ä½ ç”»ä¸ª **MoE All-to-All é€šä¿¡æµæ°´çº¿å›¾ï¼ˆPrepareIndices â†’ All-to-All â†’ LocalGatherï¼‰**ï¼ŒæŠŠè¿™äº›å‡½æ•°çš„å…³ç³»å’Œæ•°æ®æµä¸²èµ·æ¥ï¼Ÿ





# moeCommKernels.cu è§£è¯»

## é€šè¿‡åŸå­æ›´æ–°headæ¥å‘é€æ¶ˆæ¯/æ›´æ–°tailæ¥æ¥æ”¶æ¶ˆæ¯

```c
    __inline__ __device__ void updateSend()
    {
        barrier();
        if (tid == 0)
        {
            atomicAdd_system((unsigned long long*) &fifoConnInfoPtr->head, 1);
        }
        barrier();
        step++;
    }

    __inline__ __device__ void updateRecv()
    {
        barrier();
        if (tid == 0)
        {
            atomicAdd_system((unsigned long long*) &fifoConnInfoPtr->tail, 1);
        }
        barrier();
        step++;
    }
```



## load/store/åŒæ­¥ æ±‡ç¼–

```c
__device__ inline void barrier_sync(int name, int nThreads)
{
    asm volatile("barrier.sync.aligned %0, %1;" ::"r"(name), "r"(nThreads) : "memory");
}

inline __device__ void load128(uint64_t const* ptr, uint64_t& v0, uint64_t& v1)
{
    asm volatile("ld.volatile.global.v2.u64 {%0,%1}, [%2];" : "=l"(v0), "=l"(v1) : "l"(ptr) : "memory");
}

inline __device__ void store128(uint64_t* ptr, uint64_t v0, uint64_t v1)
{
    asm volatile("st.volatile.global.v2.u64 [%2], {%0,%1};" ::"l"(v0), "l"(v1), "l"(ptr) : "memory");
}
```





## è¯¦è§£

è¿™æ®µä»£ç å®ç°çš„æ˜¯ **MoE (Mixture of Experts) æ¨¡å‹ä¸­çš„ All-to-All é€šä¿¡æœºåˆ¶**ï¼Œç”¨äºåœ¨ GPU ä¸Šé«˜æ•ˆåœ°å®Œæˆ token åœ¨ä¸åŒ Expert ä¹‹é—´çš„åˆ†å‘ä¸æ”¶é›†ã€‚å®ƒåŸºäº CUDA å†…æ ¸å’Œé€šä¿¡ä¼˜åŒ–ï¼ˆwarp çº§å¯„å­˜å™¨æ“ä½œã€FIFO ç¼“å†²ã€åŸå­æ“ä½œã€barrier ç­‰ï¼‰ï¼Œä¿è¯å¤š GPU ç¯å¢ƒä¸‹çš„é«˜ååé‡æ•°æ®äº¤æ¢ã€‚ä¸‹é¢æˆ‘ç»™ä½ åˆ†å±‚è§£é‡Šï¼š

------

## 1. æ•´ä½“ç›®æ ‡

MoE è®­ç»ƒ/æ¨ç†æ—¶ï¼Œæ¯ä¸ª token éœ€è¦æ ¹æ®è·¯ç”±ï¼ˆrouter çš„ top-k é€‰æ‹©ï¼‰å‘é€åˆ°å¯¹åº”çš„ Expertã€‚
 ç”±äº Expert é€šå¸¸åˆ†å¸ƒåœ¨ä¸åŒçš„ GPU rank ä¸Šï¼Œå°±éœ€è¦ä¸€ç§é«˜æ•ˆçš„ **All-to-All é€šä¿¡**ï¼š

- **Send**ï¼šæ¯ä¸ª rank æŠŠåˆ†é…åˆ°å…¶ä»– rank çš„ token embedding å‘å‡ºå»ã€‚
- **Recv**ï¼šæ¯ä¸ª rank ä»å…¶ä»– rank æ”¶åˆ°å±äºè‡ªå·±çš„ token embeddingã€‚

ä»£ç å®ç°çš„å°±æ˜¯ä¸€ä¸ª **é€šç”¨çš„ All-to-All é€šé“ (AllToAllChannelCommunicator)**ï¼Œé€šè¿‡ FIFO é˜Ÿåˆ—å’Œ warp å¹¶è¡Œæ¥å®ç°é«˜é€Ÿé€šä¿¡ã€‚

------

## 2. æ ¸å¿ƒç»“æ„

### (1) `AllToAllChannelCommunicator<isSender>`

- æ¨¡æ¿ç±»ï¼Œåˆ†æˆ **å‘é€æ–¹ (isSender=true)** å’Œ **æ¥æ”¶æ–¹ (isSender=false)**ã€‚
- å†…éƒ¨ç»´æŠ¤ï¼š
  - `fifoConnInfoPtr`ï¼šFIFO é˜Ÿåˆ—çš„æ§åˆ¶ä¿¡æ¯ï¼ˆhead, tailï¼‰ã€‚
  - `fifoBasePtr`ï¼šFIFO ç¼“å†²åŒºçš„åŸºåœ°å€ã€‚
  - `step`ï¼šå½“å‰å¤„ç†çš„ FIFO entry æ­¥æ•°ã€‚
  - `regs[]`ï¼šçº¿ç¨‹å¯„å­˜å™¨ç¼“å­˜ï¼Œç”¨äº warp å†…æ•°æ®æ¬è¿ã€‚
  - `groupSharedBuffer`ï¼šå…±äº«å†…å­˜ï¼Œç”¨äºå­˜å‚¨å½“å‰é€šä¿¡ group çš„ç´¢å¼•èŒƒå›´ã€‚

å®ƒçš„èŒè´£å°±æ˜¯ï¼š

- åˆå§‹åŒ– FIFO ä½ç½® (`init`)
- è®¡ç®—éœ€è¦ä¼ è¾“çš„ç´¢å¼•èŒƒå›´ (`computeGroupTransferRange`)
- è½½å…¥ç´¢å¼•ã€æ˜ å°„åˆ°å®é™…æ•°æ®æŒ‡é’ˆ (`loadTransferIndices`)
- æŠŠæ•°æ®å†™å…¥ FIFO (`sendSlice`) æˆ–ä» FIFO è¯»å–æ•°æ® (`recvSlice`)
- æ›´æ–° FIFO head/tail (`updateSend` / `updateRecv`)

------

### (2) FIFO é€šä¿¡æœºåˆ¶

- FIFO æ·±åº¦ï¼š`RECV_FIFO_DEPTH = 8`ï¼Œç›¸å½“äºæµæ°´çº¿çš„ bufferã€‚
- ä¸€ä¸ª FIFO entry å¤§å°ï¼š`RECV_FIFO_ENTRY_BYTES = 256KB`ã€‚
- FIFO ç”± **å‘é€æ–¹å†™ï¼Œæ¥æ”¶æ–¹è¯»**ï¼Œé€šè¿‡ `head` / `tail` æ ‡è®°åŒæ­¥ã€‚
- å‘é€æ–¹åœ¨å†™æ•°æ®å‰ä¼šç­‰å¾… (`waitSend`)ï¼Œé˜²æ­¢è¦†ç›–æœªæ¶ˆè´¹çš„æ•°æ®ã€‚
- æ¥æ”¶æ–¹é€šè¿‡ **flag (step+1)** åˆ¤æ–­æŸä¸ªæ•°æ®æ˜¯å¦å·²ç» readyã€‚

------

### (3) Warp å¹¶è¡Œæ¬è¿

æ•°æ®æ¬è¿æ˜¯ **æŒ‰ packet (åŒ…)** ä¸ºå•ä½ï¼š

- ä¸€ä¸ª packet å¤§å°ï¼š`PACKET_SIZE_IN_U64 = WARP_SIZE * U64_DATA_REG_PER_THREAD`
- warp å†…çº¿ç¨‹åä½œï¼Œæ¯ä¸ªçº¿ç¨‹ç”¨å¯„å­˜å™¨æš‚å­˜æ•°æ®ï¼Œç„¶åå†™å…¥å…¨å±€å†…å­˜ (FIFO)ã€‚
- é‡‡ç”¨ `ld.volatile.global` å’Œ `st.volatile.global` ä¿è¯æ•°æ®å†™è¯»çš„æ—¶åºå¯è§æ€§ã€‚
- åˆ©ç”¨ `__syncwarp`ã€`barrier_sync` ä¿è¯çº¿ç¨‹é—´åŒæ­¥ã€‚

------

### (4) å‡ ä¸ªå…³é”® kernel

#### `moeAllToAllKernel`

- æ¯ä¸ª block è´Ÿè´£ä¸€ä¸ªé€šä¿¡ groupï¼ˆå‘é€/æ¥æ”¶æŸä¸€æ‰¹ tokenï¼‰ã€‚
- `blockIdx.z == 0` è¡¨ç¤º **å‘é€æ–¹**ï¼Œ`blockIdx.z == 1` è¡¨ç¤º **æ¥æ”¶æ–¹**ã€‚
- è°ƒç”¨ `AllToAllChannelCommunicator.run()` æ‰§è¡Œé€šä¿¡ã€‚

#### `computeSendRecvRankCountKernel`

- è®¡ç®—æ¯ä¸ª rank éœ€è¦å‘å…¶ä»– rank å‘é€/æ¥æ”¶å¤šå°‘ tokenã€‚
- ä½¿ç”¨ warp å†…æŠ•ç¥¨ (`tile.any`) åˆ¤æ–­æ˜¯å¦åŒ¹é…ç›®æ ‡ rankã€‚
- ç»“æœå†™å…¥ `sendRankCount` / `recvRankCount`ã€‚

#### `inplaceSendRecvRankCumSumKernel`

- å¯¹ rank çš„ token æ•°é‡åšå‰ç¼€å’Œ (prefix sum)ï¼Œå¾—åˆ°ç´¯ç§¯åç§»é‡ã€‚
- ç”¨ `cub::BlockScan` å®ç°é«˜æ•ˆå‰ç¼€å’Œã€‚

#### `computeSendRecvIndicesKernel`

- è®¡ç®—æ¯ä¸ª token çš„å®é™…ç´¢å¼•æ˜ å°„ï¼š
  - **å‘é€æ–¹**ï¼štoken çš„å®é™…åç§»å­˜åˆ° `sendRankLocalIndices`ã€‚
  - **æ¥æ”¶æ–¹**ï¼štoken çš„å…¨å±€åç§»å­˜åˆ° `recvRankLocalIndices` å’Œ `localGatherIndices`ã€‚
- è¿˜ä¼šä¿å­˜ `backwardRecvRankLocalIndices`ï¼Œç”¨äºåå‘æ¢¯åº¦èšåˆã€‚

#### `moeAllToAllMemsetKernel`

- åˆå§‹åŒ–å„ç§ index æ•°ç»„ï¼Œé¿å…åƒåœ¾æ•°æ®å½±å“åç»­è®¡ç®—ã€‚

------

## 3. è¿è¡Œæµç¨‹æ€»ç»“

ä¸€ä¸ªå®Œæ•´çš„ MoE all-to-all è¿‡ç¨‹å¤§æ¦‚æ˜¯ï¼š

1. **å‡†å¤‡é˜¶æ®µ**
   - `moeAllToAllMemset`ï¼šæ¸…é›¶ index bufferã€‚
   - `computeSendRecvRankCount`ï¼šè®¡ç®—æ¯ä¸ª rank éœ€è¦å‘/æ”¶å¤šå°‘ tokenã€‚
   - `inplaceSendRecvRankCumSum`ï¼šåš prefix sumï¼Œå¾—åˆ°ç´¯ç§¯åç§»é‡ã€‚
   - `computeSendRecvIndices`ï¼šè®¡ç®—å®é™…ç´¢å¼•æ˜ å°„å…³ç³»ã€‚
2. **é€šä¿¡é˜¶æ®µ**
   - `moeAllToAll` è°ƒç”¨ `moeAllToAllKernel`ï¼Œå¯åŠ¨å¤šä¸ª communicatorï¼š
     - **å‘é€æ–¹** æŒ‰ç´¢å¼•å†™æ•°æ®åˆ° FIFOã€‚
     - **æ¥æ”¶æ–¹** ä» FIFO è¯»å‡ºæ•°æ®ï¼Œå­˜æ”¾åˆ°æœ¬åœ° bufferã€‚
3. **æ¶ˆè´¹é˜¶æ®µ**
   - æ¯ä¸ª rank å¾—åˆ°å±äºè‡ªå·±çš„ token embeddingï¼Œäº¤ç»™å¯¹åº”çš„ Expertã€‚

------

## 4. æ€»ç»“

è¿™ä»½ä»£ç å°±æ˜¯ **MoE All-to-All é«˜æ•ˆé€šä¿¡å†…æ ¸**ï¼Œç‰¹ç‚¹ï¼š

- åŸºäº FIFO + warp packet åŒ–ä¼ è¾“ã€‚
- æ”¯æŒå¤šé€šé“ï¼ˆchannelï¼‰å¹¶è¡Œï¼Œå……åˆ†åˆ©ç”¨ GPU SMã€‚
- ç”¨ `atomicAdd_system`ã€barrier å’Œ flag æ¥ä¿è¯é€šä¿¡æ­£ç¡®æ€§ã€‚
- è¾…åŠ© kernel è´Ÿè´£è®¡ç®—ç´¢å¼•æ˜ å°„ï¼Œç¡®ä¿ token æ­£ç¡®è·¯ç”±åˆ°ç›®æ ‡ rankã€‚

------

è¦ä¸è¦æˆ‘å¸®ä½ ç”»ä¸€ä¸ª **MoE All-to-All é€šä¿¡ç¤ºæ„å›¾**ï¼Œå±•ç¤º FIFOã€sender/receiverã€warp packet çš„æ•°æ®æµï¼Ÿ



å¥½ğŸ‘Œ æˆ‘å¸®ä½ æŠŠè¿™ä¸ªå‡½æ•° **`moeCommPrepareIndicesOp`** å†ç³»ç»Ÿåœ°è§£é‡Šä¸€ä¸‹ï¼Œç»“åˆ MoEï¼ˆMixture-of-Expertsï¼‰çš„ **All-to-All é€šä¿¡** èƒŒæ™¯æ¥æ‹†è§£ï¼š

------

## ğŸ¯ å‡½æ•°ç›®æ ‡

åœ¨ **MoE åˆ†å¸ƒå¼è®­ç»ƒ** é‡Œï¼Œtoken éœ€è¦æ ¹æ®è·¯ç”±ç»“æœï¼ˆtopK expert assignmentï¼‰è¢«é‡æ–°åˆ†å‘åˆ°ä¸åŒ rank ä¸Šçš„ expertã€‚
 è¿™ä¸ªå‡½æ•°çš„ç›®æ ‡å°±æ˜¯ **å‡†å¤‡å¥½ All-to-All æ‰€éœ€çš„ç´¢å¼•ä¿¡æ¯**ï¼Œæ–¹ä¾¿åç»­ç”¨ NCCL æˆ– RDMA åšé«˜æ•ˆæ•°æ®æ¬è¿ã€‚

æœ€ç»ˆè¿”å› 6 ä¸ªå¼ é‡ï¼Œåˆ†åˆ«æè¿°äº† **æœ¬åœ°æ”¶é›†ç´¢å¼•ã€å‘é€å¸ƒå±€ã€æ¥æ”¶å¸ƒå±€ã€åå‘ç´¢å¼•**ã€‚

------

## ğŸ“¦ è¾“å…¥å‚æ•°

- `gatheredTargetRankIds`: `(N, topK)` çš„ int32 çŸ©é˜µï¼Œè¡¨ç¤ºæ¯ä¸ª token è·¯ç”±åˆ°çš„ç›®æ ‡ rank IDï¼ˆå¯èƒ½å¤šä¸ªï¼Œå› ä¸º topKï¼‰ã€‚
- `realRankTokenCountCumSum`: optional `(epSize,)`ï¼Œè¡¨ç¤ºæ¯ä¸ª rank çš„ç´¯è®¡ token æ•°å‰ç¼€å’Œï¼ˆå¯ç”¨äºå˜é•¿æƒ…å†µï¼‰ã€‚
- `maxTokenCountPerRank`: æ¯ä¸ª rank èƒ½å¤„ç†çš„æœ€å¤§ token æ•°ï¼ˆslot å¤§å°ï¼‰ã€‚
- `expertCount`: MoE expert æ€»æ•°ã€‚
- `topK`: æ¯ä¸ª token é€‰æ‹©çš„ä¸“å®¶æ•°ã€‚
- `epRank`: å½“å‰ EPï¼ˆExpert Parallelismï¼‰world çš„ rank idã€‚
- `epSize`: EP world æ€»å¤§å°ã€‚

------

## âš™ï¸ æ ¸å¿ƒé€»è¾‘

### 1. è¾“å…¥æ£€æŸ¥

ç¡®ä¿ï¼š

- `gatheredTargetRankIds` æ˜¯ `(?, topK)`ã€‚
- `realRankTokenCountCumSum`ï¼ˆå¦‚æœæœ‰ï¼‰æ˜¯ä¸€ç»´ int32ï¼Œé•¿åº¦ç­‰äº `epSize`ã€‚
- å„ç§å‚æ•°èŒƒå›´åˆæ³•ã€‚

### 2. å†…å­˜åˆ†é…

ç”³è¯· 6 ä¸ªè¾“å‡ºå¼ é‡ï¼ˆå…¨æ˜¯ `int32`ï¼‰ï¼š

1. `localGatherIndices`
   - shape = `(maxTokenCountPerRank * epSize,)`
   - è¡¨ç¤º **æœ¬åœ° rank ä»è¾“å…¥ä¸­è¦ gather çš„ token ç´¢å¼•**ã€‚
2. `sendRankCountCumSum`
   - shape = `(epSize,)`
   - æ¯ä¸ª rank éœ€è¦å‘é€å¤šå°‘ token çš„ç´¯è®¡å’Œï¼ˆç±»ä¼¼ prefix sumï¼‰ã€‚
3. `sendRankLocalIndices`
   - shape = `(maxTokenCountPerRank * maxSendRanksPerToken,)`
   - è¡¨ç¤º **æœ¬ rank è¦å‘é€ç»™æ¯ä¸ªç›®æ ‡ rank çš„ token ç´¢å¼•**ã€‚
4. `recvRankCountCumSum`
   - shape = `(epSize,)`
   - æ¯ä¸ª rank å°†è¦æ¥æ”¶å¤šå°‘ token çš„ç´¯è®¡å’Œã€‚
5. `recvRankLocalIndices`
   - shape = `(maxTokenCountPerRank * epSize,)`
   - æœ¬ rank æ¥æ”¶åˆ°çš„ token åœ¨æœ¬åœ° buffer é‡Œçš„ç´¢å¼•ã€‚
6. `backwardRecvRankLocalIndices`
   - shape = `(maxTokenCountPerRank * maxSendRanksPerToken,)`
   - **åå‘ä¼ æ’­ç”¨çš„æ˜ å°„è¡¨**ï¼Œä¿è¯æ¢¯åº¦èƒ½å¯¹åº”å›å»ã€‚

### 3. è°ƒç”¨ CUDA kernel

è°ƒç”¨ `tensorrt_llm::kernels::moeAllToAllPrepareIndices`ï¼š

```cpp
moeAllToAllPrepareIndices(
    worldInfo, expertParallelInfo, maxTokenCountPerRank,
    gatheredTargetRankIds.data_ptr<int>(),
    realRankTokenCountCumSumPtr,
    localGatherIndices.data_ptr<int>(),
    sendRankCountCumSum.data_ptr<int>(),
    sendRankLocalIndices.data_ptr<int>(),
    recvRankCountCumSum.data_ptr<int>(),
    recvRankLocalIndices.data_ptr<int>(),
    backwardRecvRankLocalIndices.data_ptr<int>(),
    stream
);
```

è¿™ä¸ª kernel è´Ÿè´£ï¼š

- **åˆ†æ token çš„ç›®æ ‡ rank**
- **è®¡ç®—å‘é€/æ¥æ”¶çš„åˆ†å¸ƒ**
- **å†™å¥½ gather / scatter çš„ç´¢å¼•è¡¨**

------

## ğŸ“¤ è¿”å›å€¼

è¿”å›ä¸€ä¸ª tupleï¼š

```
(localGatherIndices,
 sendRankCountCumSum,
 sendRankLocalIndices,
 recvRankCountCumSum,
 recvRankLocalIndices,
 backwardRecvRankLocalIndices)
```

------

## ğŸ§© ç±»æ¯”ç†è§£

ä½ å¯ä»¥æŠŠå®ƒç±»æ¯”æˆ **ä¸€æ¬¡å¤§è§„æ¨¡åŒ…è£¹è½¬è¿**ï¼š

- `gatheredTargetRankIds`: æ¯ä¸ªåŒ…è£¹ä¸Šå†™äº†â€œè¦å¯„å¾€å“ªä¸ªä»“åº“â€ã€‚
- `localGatherIndices`: æœ¬ä»“åº“è¦å…ˆä»å †é‡ŒæŒ‘å‡ºå“ªäº›åŒ…è£¹ã€‚
- `sendRankCountCumSum` & `sendRankLocalIndices`: æœ¬ä»“åº“è¦å‘å¤šå°‘åŒ…è£¹ï¼Œå‘ç»™è°ã€‚
- `recvRankCountCumSum` & `recvRankLocalIndices`: æœ¬ä»“åº“ä¼šæ”¶åˆ°å¤šå°‘åŒ…è£¹ï¼Œæ”¾åˆ°å“ªé‡Œã€‚
- `backwardRecvRankLocalIndices`: ä¿è¯é€€è´§æ—¶è¿˜èƒ½é€å›åŸä¸»ã€‚

------

è¦ä¸è¦æˆ‘å¸®ä½ ç”»ä¸€ä¸ª **å›¾ç¤ºä¾‹**ï¼ˆæ¯”å¦‚ `epSize=2, topK=2, maxTokenCountPerRank=4` çš„å°ä¾‹å­ï¼‰ï¼Œç›´è§‚å±•ç¤ºä¸€ä¸‹è¿™å‡ ä¸ªå¼ é‡é‡Œåˆ†åˆ«é•¿å•¥æ ·ï¼Ÿ





